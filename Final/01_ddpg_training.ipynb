{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Gymnasium imports\n",
    "import gymnasium as gym \n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "# Import helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# Import stable baselines\n",
    "from stable_baselines3 import PPO, A2C, DDPG\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Import custom classes\n",
    "from Environment.env_v9 import *\n",
    "from Functions.visualization_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the network\n",
    "with open('./Config/network_config_v1.json') as file:\n",
    "    network_config = file.read()\n",
    "\n",
    "EP_LENGTH = 100 # Length of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def make_env(config_file=\"config.json\"):\n",
    "\n",
    "    config = load_config(config_file)\n",
    "\n",
    "    env = SS_Mngmt_Env(\n",
    "        network_config=network_config,\n",
    "        EP_LENGTH=EP_LENGTH,\n",
    "        render_mode=\"human\",\n",
    "        model_type=\"DDPG\",\n",
    "        stockout_cost=config[\"stockout_cost\"],\n",
    "        order_cost=config[\"order_cost\"],\n",
    "        item_cost=config[\"item_cost\"],\n",
    "        stock_cost=config[\"stock_cost\"],\n",
    "        item_prize=config[\"item_prize\"],\n",
    "        progressive_stock_cost=config[\"progressive_stock_cost\"],\n",
    "        stock_out_max=config[\"stock_out_max\"],\n",
    "        order_quantities=config[\"order_quantities\"],\n",
    "        demand_mean=config[\"demand_mean\"],\n",
    "        demand_std=config[\"demand_std\"],\n",
    "        demand_noise=config[\"demand_noise\"],\n",
    "        demand_noise_std=config[\"demand_noise_std\"],\n",
    "        demand_prob=config[\"demand_prob\"],\n",
    "    )\n",
    "\n",
    "    # Wrap the environment with the monitor and for box actions\n",
    "    return Monitor(MultiDiscreteToBoxWrapper(env))\n",
    "\n",
    "env = make_env(\"./Config/env_config_v0.json\")\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = DummyVecEnv([lambda: make_env(\"./Config/env_config_v0.json\")])\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "model_path = os.path.join('Training', 'Models', 'DDPG')\n",
    "log_path = os.path.join('Training', 'Logs', 'DDPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparams(trial):\n",
    "    # Suggest the most important hyperparameters\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    tau = trial.suggest_float(\"tau\", 1e-3, 1e-1, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
    "    buffer_size = trial.suggest_int(\"buffer_size\", 50000, 500000, step=50000)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    noise_std = trial.suggest_float(\"noise_std\", 0.1, 0.5)\n",
    "\n",
    "    # Define action noise\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions))\n",
    "\n",
    "    # Create the DDPG model with static and optimized parameters\n",
    "    model = DDPG(\n",
    "        \"MultiInputPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        learning_rate=lr,  # Optimized\n",
    "        buffer_size=buffer_size,  # Optimized\n",
    "        batch_size=batch_size,  # Optimized\n",
    "        tau=tau,  # Optimized\n",
    "        gamma=gamma,  # Optimized\n",
    "        train_freq=(1, \"episode\"),  # Static\n",
    "        gradient_steps=1,  # Static\n",
    "        learning_starts=100,  # Static\n",
    "        tensorboard_log=log_path,  # Static\n",
    "        action_noise=action_noise,  # Optimized\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=200_000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    rewards, _ = evaluate_policy(model, env, n_eval_episodes=5, return_episode_rewards=True)\n",
    "\n",
    "    # Return the average reward as the objective to maximize\n",
    "    return sum(rewards) / len(rewards)\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optimize_hyperparams, n_trials=25)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "# Save the study to a file\n",
    "with open(f\"./Training/PPO_optuna_study_{now.strftime(\"%Y-%m-%d_%H_%M\")}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-scm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
