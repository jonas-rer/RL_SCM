{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.460661Z","iopub.status.busy":"2024-12-03T16:44:16.459732Z","iopub.status.idle":"2024-12-03T16:44:16.468888Z","shell.execute_reply":"2024-12-03T16:44:16.467572Z","shell.execute_reply.started":"2024-12-03T16:44:16.460613Z"},"trusted":true},"outputs":[],"source":["# Imports\n","# Gymnasium imports\n","import gymnasium as gym \n","from gymnasium import Env\n","from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n","\n","import networkx as nx\n","from networkx.drawing.nx_agraph import graphviz_layout\n","\n","# Import helpers\n","import numpy as np\n","import pandas as pd\n","import random\n","import os\n","import json\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import optuna\n","import pickle\n","import csv\n","\n","from datetime import datetime\n","\n","from collections import deque\n","\n","# Import stable baselines\n","from stable_baselines3 import PPO, A2C, DDPG\n","from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\n","from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.env_checker import check_env\n","from stable_baselines3.common.noise import NormalActionNoise\n","\n","# Import tensorflow\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.479244Z","iopub.status.busy":"2024-12-03T16:44:16.478828Z","iopub.status.idle":"2024-12-03T16:44:16.557542Z","shell.execute_reply":"2024-12-03T16:44:16.556353Z","shell.execute_reply.started":"2024-12-03T16:44:16.479205Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class SS_Mngmt_Env(Env):\n","    \"\"\"\n","    Supply Chain Management Environment\n","    Environment for supply chain management with a single product and multiple nodes.\n","    The action space constists of the order quantities for each node.\n","    The observation space consists of the inventory levels and the planned and actual demand for each node.\n","    \"\"\"\n","\n","    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n","\n","    # Define the action and observation space\n","    def __init__(\n","        self,\n","        EP_LENGTH=52,\n","        network_config=None,\n","        render_mode=None,\n","        model_type=None,\n","        stockout_cost=500,  # Cost of stockout\n","        stock_out_max=9,  # Maximum number of stockouts\n","        order_cost=5,  # Cost of each order\n","        item_cost=0.1,  # Cost of each item\n","        stock_cost=0.5,  # Cost of stock per unit\n","        item_prize=20,  # Prize of each item\n","        order_quantities=[0, 15, 50],  # Order quantities for each node\n","        demand_mean=10,  # Mean demand\n","        demand_std=2,  # Standard deviation of demand\n","        demand_noise=0,  # Mean noise in demand\n","        demand_noise_std=2,  # Standard deviation of noise in demand\n","        demand_prob=0.4,  # Probability of having demand\n","        progressive_stock_cost=False,  # Progressive stock cost\n","        kaggle=False,  # Kaggle mode (True or False)\n","    ):\n","        \"\"\"\n","        Initialize the environment\n","        EP_LENGTH: int - Total length of the episode\n","        network_config: str - JSON string with network configuration\n","        render_mode: str - Render mode for the environment\n","        model_type: str - Type of model (e.g., PPO, A2C)\n","        stockout_cost: float - Cost of stockout\n","        stock_out_max: int - Maximum number of stockouts\n","        order_cost: float - Cost of each order\n","        item_cost: float - Cost of each item\n","        stock_cost: float - Cost of stock per unit\n","        item_prize: float - Prize of each item\n","        order_quantities: list - Order quantities for each node\n","        demand_mean: float - Mean demand\n","        demand_std: float - Standard deviation of demand\n","        demand_noise: float - Mean noise in demand\n","        demand_noise_std: float - Standard deviation of noise in demand\n","        demand_prob: float - Probability of having demand\n","        progressive_stock_cost: bool - Progressive stock cost\n","        kaggle: bool - Kaggle mode (True or False)\n","        \"\"\"\n","\n","        self.EP_LENGTH = EP_LENGTH  # Total length\n","        self.episode_length = EP_LENGTH  # Current length of the episode\n","\n","        self.total_reward = 0\n","\n","        self.model_type = model_type\n","\n","        # Set the data path\n","        now = datetime.now()\n","        self.data_path = (\n","            f'/kaggle/working/{now.strftime(\"%Y-%m-%d\")}_environment_data_{self.model_type}.csv'\n","        )\n","\n","        # Set the fieldnames for the CSV file\n","        self.fieldnames = [\n","            \"Time\",\n","            \"Node\",\n","            \"Stock\",\n","            \"Action\",\n","            \"Demand\",\n","            \"Delivery\",\n","            \"Reward\",\n","            \"Total Reward\",\n","            \"Backlog\",\n","        ]\n","\n","        self.file_initialized = False\n","\n","        # Seting up the network\n","        self.network_config = network_config\n","        self.graph = nx.DiGraph()\n","        self.setup_network(self.network_config)\n","\n","        self.lead_times = nx.get_edge_attributes(self.graph, \"L\")\n","\n","        # Number of nodes excluding 'S' and 'D'\n","        num_nodes = len(self.graph.nodes) - 2\n","\n","        # Define the costs\n","        self.stockout_cost = stockout_cost\n","        self.order_cost = order_cost\n","        self.item_cost = item_cost\n","        self.stock_cost = stock_cost\n","        self.stock_out_max = stock_out_max\n","        self.item_prize = item_prize\n","        self.progressive_stock_cost = progressive_stock_cost\n","\n","        self.stock_out_counter = 0\n","\n","        self.order_quantities = order_quantities\n","\n","        # Order delay and queue\n","        self.order_queues = self.order_queue(initial_order=order_quantities[1])\n","\n","        # Backlog queue for each node\n","        self.backlog_queues = self.backlog_queue()\n","\n","        # Define action space\n","        n_actions = len(order_quantities)\n","        n_nodes = len(self.graph.nodes) - 2\n","        action_choices = np.full(n_nodes, n_actions)\n","        self.action_space = MultiDiscrete(action_choices)\n","\n","        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n","        self.observation_space = Dict(\n","            {\n","                \"inventory_levels\": Box(\n","                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n","                ),\n","                \"current_demand\": Box(\n","                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n","                ),\n","                \"backlog_levels\": Box(\n","                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n","                ),\n","                \"order_queues\": Box(\n","                    low=0, high=1000, shape=(num_nodes, max_lead_time), dtype=np.float32\n","                ),\n","                \"lead_times\": Box(\n","                    low=1, high=max_lead_time, shape=(num_nodes,), dtype=np.int32\n","                ),\n","            }\n","        )\n","\n","        # Setting up the initial state\n","        self.demand_mean = demand_mean\n","        self.demand_std = demand_std\n","        self.demand_noise = demand_noise\n","        self.demand_noise_std = demand_noise_std\n","        self.demand_prob = demand_prob\n","\n","        self.planned_demands = self.planned_demand(\n","            self.demand_mean, self.demand_std, self.demand_prob\n","        )\n","        self.actual_demands = self.actual_demand(\n","            self.planned_demands, self.demand_noise_std, self.demand_noise\n","        )\n","\n","        # Collect initial inventories from the graph\n","        initial_inventories = []\n","        for node in self.graph.nodes:\n","            if node not in [\"S\", \"D\"]:\n","                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n","\n","        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n","\n","        self.state = {\n","            \"inventory_levels\": initial_inventories.astype(np.float32),\n","            \"planned_demand\": self.planned_demands,\n","            \"actual_demand\": self.actual_demands,\n","            \"current_demand\": self.actual_demands[0],\n","            \"backlog_levels\": np.zeros(num_nodes),\n","            \"order_queue_status\": np.zeros(num_nodes),\n","        }\n","\n","        # Prep to save the data\n","        self.inventory = initial_inventories\n","        self.stock_history = [self.inventory.tolist()]\n","        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n","\n","        # Kaggle mode\n","        self.kaggle = kaggle\n","\n","        # Render mode\n","        self.render_mode = render_mode\n","        self.screen_initialized = False\n","\n","    # Defining the step function\n","    def step(self, action):\n","        \"\"\"\n","        Executes one step in the environment.\n","        Starts by processing the orders and updating the inventory levels for each node.\n","        Then, it computes the reward based on the order costs and stock level.\n","        Finally, it checks if the episode is done and returns the next state, reward, and whether the episode is done.\n","        \"\"\"\n","\n","        # Returns the next state, reward and whether the episode is done\n","        timestep = self.EP_LENGTH - self.episode_length\n","\n","        # num_nodes = len(self.graph.nodes) - 2\n","\n","        # Retrieve the current inventory levels\n","        self.inventory = self.state[\"inventory_levels\"]\n","        inventory_levels = np.copy(self.inventory)\n","        reward = 0\n","\n","        # Retrieve the actual demand for the current timestep\n","        self.current_demand = self.actual_demands[timestep].astype(np.float32)\n","\n","        # Add every first element of the order queues to the history\n","        self.new_order = [self.order_quantities[i] for i in action]\n","\n","        # For visualization and history data\n","        self.orders = np.array(\n","            [\n","                self.order_queues[node][0]\n","                for node in self.graph.nodes\n","                if node not in [\"S\", \"D\"]\n","            ]\n","        )\n","\n","        # Process the orders and update the inventory levels for each node\n","        for node in self.graph.nodes:\n","            if node not in [\"S\", \"D\"]:\n","                node_index = self.node_to_index(node)\n","\n","                # Deduct costs for placing new orders\n","                if self.new_order[node_index] > 0:\n","                    reward -= self.order_cost + (\n","                        self.new_order[node_index] * self.item_cost\n","                    )\n","\n","                # Fulfill orders from the queue\n","                order = self.order_queues[node].popleft()\n","                inventory_levels[node_index] += order\n","\n","                # Attempt to meet current demand\n","                node_demand = self.current_demand[node_index]\n","                if inventory_levels[node_index] >= node_demand:\n","                    # Enough stock to meet demand\n","                    inventory_levels[node_index] -= node_demand\n","                    reward += node_demand * self.item_prize\n","                else:\n","                    # Insufficient stock - add unmet demand to backlog and apply penalty\n","                    unmet_demand = node_demand - inventory_levels[node_index]\n","                    inventory_levels[node_index] -= node_demand - unmet_demand\n","                    reward += (node_demand - unmet_demand) * self.item_prize\n","                    reward -= self.stockout_cost * unmet_demand  # Apply stockout cost\n","                    self.backlog_queues[node].append(unmet_demand)\n","\n","                    # Increment the stockout counter\n","                    self.stock_out_counter += 1\n","\n","                # Process backlog with any remaining stock\n","                while self.backlog_queues[node] and inventory_levels[node_index] > 0:\n","                    backlog_demand = self.backlog_queues[node][0]\n","                    if inventory_levels[node_index] >= backlog_demand:\n","                        inventory_levels[node_index] -= backlog_demand\n","                        reward += backlog_demand * self.item_prize\n","                        self.backlog_queues[node].popleft()\n","                    else:\n","                        break  # Not enough stock to clear the backlog completely\n","\n","                # backlog penalty\n","                reward -= self.stockout_cost * len(self.backlog_queues[node])\n","\n","                # Replenish order queue\n","                self.order_queues[node].append(self.new_order[node_index])\n","\n","        if self.progressive_stock_cost == False:\n","            # Compute the reward based on the order costs and stock level\n","            reward -= np.sum(inventory_levels) * self.stock_cost\n","        elif self.progressive_stock_cost == True:\n","            reward -= np.sum(\n","                [\n","                    self.quadratic_stock_cost(self.stock_cost, inv)\n","                    for inv in inventory_levels\n","                ]\n","            )\n","\n","        # Penalty if the episode cannot be completed\n","        if self.stock_out_counter >= self.stock_out_max:\n","            reward -= (\n","                self.episode_length * self.stockout_cost * (len(self.graph.nodes) - 2)\n","            )\n","\n","        # Update the reward\n","        self.total_reward += reward\n","\n","        # Decrease the episode length\n","        self.episode_length -= 1\n","\n","        inventory_levels = inventory_levels.flatten()\n","        self.inventory = inventory_levels\n","\n","        self.state = {\n","            \"inventory_levels\": inventory_levels.astype(np.float32),\n","            \"planned_demand\": self.planned_demands,\n","            \"actual_demand\": self.actual_demands,\n","            \"current_demand\": self.actual_demands[timestep],\n","            \"backlog_levels\": self.backlog_queues,\n","            \"order_queue_status\": self.order_queues,\n","        }\n","\n","        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n","        obs = {\n","            \"inventory_levels\": self.inventory.astype(np.float32),\n","            \"current_demand\": self.actual_demands[timestep].astype(np.float32),\n","            \"backlog_levels\": np.array(\n","                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n","            ),\n","            \"order_queues\": np.array(\n","                [\n","                    list(self.order_queues[node])\n","                    + [0] * (max_lead_time - len(self.order_queues[node]))\n","                    for node in self.graph.nodes\n","                    if node not in [\"S\", \"D\"]\n","                ],\n","                dtype=np.float32,\n","            ),\n","            \"lead_times\": np.array(\n","                [\n","                    len(self.order_queues[node])\n","                    for node in self.graph.nodes\n","                    if node not in [\"S\", \"D\"]\n","                ],\n","                dtype=np.int32,\n","            ),\n","        }\n","\n","        # Update the history data\n","        self.reward_history.append(reward)\n","        self.stock_history.append(list(self.inventory))\n","\n","        self.log_step_data(timestep, action, reward)\n","\n","        # Check if the episode is done\n","        done = self.episode_length == 0\n","\n","        # Check if episode is done\n","        if self.stock_out_counter >= self.stock_out_max:\n","\n","            done = True\n","\n","        elif self.episode_length <= 0:\n","\n","            done = True\n","\n","        else:\n","\n","            done = False\n","\n","        # Set placeholder for info\n","        info = {}\n","\n","        # Check if the episode is truncated\n","        truncated = False\n","\n","        return obs, float(reward), done, truncated, info\n","\n","    def quadratic_stock_cost(self, stock_cost, inventory_level):\n","        \"\"\"\n","        Quadratic stock cost function.\n","        \"\"\"\n","        return stock_cost * (inventory_level**2)  # Quadratic cost\n","\n","    def log_step_data(self, timestep, action, reward):\n","\n","        if not self.file_initialized:\n","            with open(self.data_path, \"w\", newline=\"\") as csvfile:\n","                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n","                writer.writeheader()\n","            self.file_initialized = True  # Mark as initialized\n","\n","        for n in range(len(self.inventory)):\n","            node_name = self.get_node_name(n)\n","            row = {\n","                \"Time\": timestep + 1,\n","                \"Node\": node_name,\n","                \"Stock\": self.inventory[n],\n","                \"Action\": self.new_order[n],\n","                \"Demand\": self.current_demand[n],\n","                \"Delivery\": self.orders[n],\n","                \"Reward\": reward,\n","                \"Total Reward\": self.total_reward,\n","                \"Backlog\": len(self.backlog_queues[node_name]) > 0,\n","            }\n","\n","            # Append the row to the CSV file\n","            with open(self.data_path, \"a\", newline=\"\") as csvfile:\n","                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n","                writer.writerow(row)\n","\n","    def reward_function(self):\n","        # TODO - Implement a custom reward function\n","\n","        return 0\n","\n","    def render(self):\n","        # Just check episode lenghth and only plot the last one when using matplotlib\n","        if self.render_mode is not None:\n","            if self.render_mode == \"human\":\n","                self.render_human()\n","\n","    def render_human(self):\n","        \"\"\"\n","        Renders the environment in human mode.\n","        Useful for debugging and visualization.\n","        \"\"\"\n","\n","        print(\"*\" * 50)\n","        print(\"\\nEpisode Information\")\n","        print(f\"Episode Length: {self.EP_LENGTH - self.episode_length}\")\n","        if len(self.stock_history) > 1:\n","            print(f\"Stock Level (Previous Timestep): {self.stock_history[-2]}\")\n","        else:\n","            print(\n","                \"Stock Level (Previous Timestep): No previous timestep data available\"\n","            )\n","        print(f\"Stock Level: {self.inventory}\")\n","        print(\n","            f\"Planned Demand: {self.planned_demands[self.EP_LENGTH - self.episode_length - 1]}\"\n","        )\n","        print(f\"Actual Demand: {self.current_demand}\")\n","        print(f\"Action: {self.new_order}\")\n","        print(f\"Deliveries: {self.orders}\")\n","        # print(\n","        #     f\"Previous Reward: {self.reward_history[self.EP_LENGTH - self.episode_length - 1]}\"\n","        # )\n","        print(f\"Step Reward: {self.reward_history[-1]}\")\n","        print(f\"Total Reward: {self.total_reward}\")\n","\n","        print(\"\\nBacklog:\")\n","        print([len(queue) > 0 for queue in self.backlog_queues.values()])\n","        # pprint(self.backlog_queues, indent=4)\n","\n","        print(\"\\nOrder Queue:\")\n","        pprint(self.order_queues, indent=4)\n","        print()\n","\n","        # print(\"Stockout Cost: \", self.stockout_cost)\n","        print(\"\\nStockout Counter: \", self.stock_out_counter)\n","\n","        return\n","\n","    def setup_network(self, network_config=None):\n","        \"\"\"\n","        Sets up the network graph based on the configuration provided.\n","        \"\"\"\n","        config = json.loads(network_config)\n","\n","        # Add nodes to the graph\n","        for node, attributes in config[\"nodes\"].items():\n","            self.graph.add_node(node, **attributes)\n","\n","        # Add edges to the graph with lead times\n","        for edge in config[\"edges\"]:\n","            self.graph.add_edge(edge[\"source\"], edge[\"target\"], L=edge[\"L\"])\n","\n","    def render_network(self):\n","        \"\"\"\n","        Renders the network graph using NetworkX and Matplotlib.\n","        \"\"\"\n","\n","        print(\"Node Attributes:\")\n","        for node, attributes in self.graph.nodes(data=True):\n","            print(f\"Node {node}: {attributes}\")\n","\n","        pos = graphviz_layout(self.graph, prog=\"dot\")\n","\n","        plt.figure(figsize=(8, 6))\n","\n","        nx.draw_networkx_nodes(self.graph, pos, node_size=700, node_color=\"lightblue\")\n","        nx.draw_networkx_edges(\n","            self.graph, pos, edgelist=self.graph.edges(), arrowstyle=\"->\", arrowsize=20\n","        )\n","        nx.draw_networkx_labels(self.graph, pos, font_size=12, font_family=\"sans-serif\")\n","\n","        edge_labels = nx.get_edge_attributes(self.graph, \"L\")\n","        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n","\n","        plt.title(\"Supply Chain Network Graph\", fontsize=15)\n","\n","        # Display the plot\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","    def node_to_index(self, node):\n","        \"\"\"\n","        Returns the index of the node given its name.\n","        \"\"\"\n","        return list(self.graph.nodes).index(node)\n","\n","    def get_node_name(self, index):\n","        \"\"\"\n","        Returns the name of the node given its index.\n","        \"\"\"\n","        return list(self.graph.nodes)[index]\n","\n","    def planned_demand(self, demand_mean=10, demand_std=2, demand_prob=0.8):\n","        \"\"\"\n","        Generates planned demand for each edge in the network over the whole episode.\n","        \"\"\"\n","\n","        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n","\n","        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n","\n","        for i, edge in enumerate(edges_leading_to_D):\n","            for j in range(self.EP_LENGTH):\n","                # Introduce a probability of having demand\n","                if np.random.rand() < demand_prob:\n","                    planned_demand[j, i] = int(\n","                        np.random.normal(demand_mean, demand_std)\n","                    )\n","\n","        return planned_demand\n","\n","    def planned_demand_even(self, demand_mean, demand_std):\n","        \"\"\"\n","        Generates planned demand for each edge in the network over the whole episode.\n","        The demand is distributed evenly, occurring only at fixed intervals (e.g., every fifth timestep).\n","        \"\"\"\n","\n","        # Get edges leading to \"D\"\n","        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n","\n","        # Initialize demand array with zeros\n","        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n","\n","        for i, edge in enumerate(edges_leading_to_D):\n","            # Determine timesteps where demand occurs (e.g., every fifth timestep)\n","            timesteps_with_demand = np.arange(0, self.EP_LENGTH, 5)\n","\n","            for j in timesteps_with_demand:\n","                # Generate demand from a normal distribution\n","                demand = max(\n","                    0, np.random.normal(demand_mean, demand_std)\n","                )  # Ensure non-negative demand\n","                planned_demand[j, i] = int(demand)\n","\n","        return planned_demand\n","\n","    def actual_demand(self, planned_demand, demand_noise_std, demand_noise):\n","        \"\"\"\n","        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n","        \"\"\"\n","\n","        actual_demand = np.copy(planned_demand)\n","\n","        for i in range(actual_demand.shape[0]):\n","            for j in range(actual_demand.shape[1]):\n","                # Add a small random noise to the planned demand\n","                if planned_demand[i, j] > 0:\n","                    noise = np.random.normal(demand_noise, demand_noise_std)\n","                    # Ensure actual demand is not less than 0\n","                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n","\n","        return actual_demand\n","\n","    def actual_demand_extremes(self, planned_demand, demand_noise_std, demand_noise):\n","        \"\"\"\n","        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n","        The to create a more interesting scenario the demand is multiplied by a random factor every now and then.\n","        \"\"\"\n","\n","        actual_demand = np.copy(planned_demand)\n","\n","        for i in range(actual_demand.shape[0]):\n","            for j in range(actual_demand.shape[1]):\n","                # Add a small random noise to the planned demand\n","                if planned_demand[i, j] > 0:\n","                    noise = np.random.normal(demand_noise, demand_noise_std)\n","                    # Ensure actual demand is not less than 0\n","                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n","\n","        return actual_demand\n","\n","    def order_queue(self, initial_order=10):\n","        \"\"\"\n","        Creates a dictionary for the order queues for each node in the network.\n","        \"\"\"\n","\n","        order_queues = {}\n","\n","        for node in self.graph.nodes:\n","\n","            if node not in [\"S\", \"D\"]:\n","                in_edges = list(self.graph.in_edges(node, data=True))\n","\n","                if in_edges:\n","                    lead_time = in_edges[0][2][\"L\"]\n","                    order_queues[node] = deque(\n","                        [initial_order] + [0] * (lead_time - 1), maxlen=lead_time\n","                    )\n","\n","        return order_queues\n","\n","    def backlog_queue(self):\n","        \"\"\"\n","        Creates a dictionary for the backlog queues for each node in the network.\n","        \"\"\"\n","\n","        backlog_queues = {}\n","\n","        for node in self.graph.nodes:\n","            if node not in [\"S\", \"D\"]:\n","\n","                in_edges = list(self.graph.in_edges(node, data=True))\n","                if in_edges:\n","                    backlog_queues[node] = deque()\n","\n","        return backlog_queues\n","\n","    def save_state(self):\n","        \"\"\"\n","        Saves the current state of the environment.\n","        Used for greedy algorithm.\n","        \"\"\"\n","\n","        return {\n","            \"episode_length\": self.episode_length,\n","            \"inventory\": np.copy(self.inventory),\n","            \"total_reward\": self.total_reward,\n","            \"state\": self.state,\n","            \"order_queues\": {k: deque(v) for k, v in self.order_queues.items()},\n","            \"backlog_queues\": {k: deque(v) for k, v in self.backlog_queues.items()},\n","        }\n","\n","    def load_state(self, saved_state):\n","        \"\"\"\n","        Loads the state of the environment.\n","        Used for greedy algorithm.\n","        \"\"\"\n","\n","        self.episode_length = saved_state[\"episode_length\"]\n","        self.inventory = saved_state[\"inventory\"]\n","        self.total_reward = saved_state[\"total_reward\"]\n","        self.state = saved_state[\"state\"]\n","        self.order_queues = {\n","            k: deque(v) for k, v in saved_state[\"order_queues\"].items()\n","        }\n","        self.backlog_queues = {\n","            k: deque(v) for k, v in saved_state[\"backlog_queues\"].items()\n","        }\n","\n","    def reset(self, seed=None, options=None):\n","        \"\"\"\n","        Resets the environment to the initial state.\n","        \"\"\"\n","        super().reset(seed=seed)  # Reset the seed\n","        if seed is not None:\n","            random.seed(seed)\n","\n","        # Reset the episode length\n","        self.episode_length = self.EP_LENGTH\n","\n","        self.file_initialized = False\n","\n","        self.total_reward = 0\n","\n","        # Reset the network\n","        self.graph = nx.DiGraph()\n","        self.setup_network(self.network_config)\n","\n","        num_nodes = len(self.graph.nodes) - 2\n","\n","        # Order delay and backlog queue\n","        self.order_queues = self.order_queue(initial_order=self.order_quantities[1])\n","        self.backlog_queues = self.backlog_queue()\n","\n","        self.stock_out_counter = 0\n","\n","        # Define the initial state\n","        self.planned_demands = self.planned_demand(\n","            self.demand_mean, self.demand_std, self.demand_prob\n","        ).astype(np.float32)\n","\n","        self.actual_demands = self.actual_demand(\n","            self.planned_demands, self.demand_noise_std, self.demand_noise\n","        ).astype(np.float32)\n","\n","        self.current_demand = self.actual_demands[0].astype(np.float32)\n","\n","        # Collect initial inventories from the graph\n","        initial_inventories = []\n","        for node in self.graph.nodes:\n","            if node not in [\"S\", \"D\"]:\n","                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n","\n","        # Convert to numpy array\n","        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n","\n","        self.state = {\n","            \"inventory_levels\": initial_inventories,\n","            \"planned_demand\": self.planned_demands,\n","            \"actual_demand\": self.current_demand,\n","        }\n","\n","        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n","        obs = {\n","            \"inventory_levels\": self.inventory.astype(np.float32),\n","            \"current_demand\": self.actual_demands[0].astype(np.float32),\n","            \"backlog_levels\": np.array(\n","                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n","            ),\n","            \"order_queues\": np.array(\n","                [\n","                    list(self.order_queues[node])\n","                    + [0] * (max_lead_time - len(self.order_queues[node]))\n","                    for node in self.graph.nodes\n","                    if node not in [\"S\", \"D\"]\n","                ],\n","                dtype=np.float32,\n","            ),\n","            \"lead_times\": np.array(\n","                [\n","                    len(self.order_queues[node])\n","                    for node in self.graph.nodes\n","                    if node not in [\"S\", \"D\"]\n","                ],\n","                dtype=np.int32,\n","            ),\n","        }\n","\n","        # Resetting history data\n","        self.inventory = initial_inventories\n","        self.stock_history = [self.inventory.tolist()]\n","        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n","\n","        # Placeholder for info\n","        info = {}\n","\n","        return obs, info\n","\n","\n","class MultiDiscreteToBoxWrapper(gym.ActionWrapper):\n","    \"\"\"\n","    Converts a MultiDiscrete action space to a Box action space.\n","    \"\"\"\n","\n","    def __init__(self, env):\n","        super().__init__(env)\n","        assert isinstance(\n","            env.action_space, MultiDiscrete\n","        ), \"Environment must have a MultiDiscrete action space.\"\n","        self.original_action_space = env.action_space\n","        self.action_space = Box(\n","            low=0.0,\n","            high=1.0,\n","            shape=self.original_action_space.nvec.shape,\n","            dtype=np.float32,\n","        )\n","\n","    def action(self, action):\n","        # Convert continuous action (Box) back to discrete (MultiDiscrete)\n","        discrete_action = np.round(\n","            action * (self.original_action_space.nvec - 1)\n","        ).astype(int)\n","        return discrete_action\n","\n","    def reverse_action(self, action):\n","        # Optionally: Map discrete actions back to normalized (Box)\n","        normalized_action = action / (self.original_action_space.nvec - 1)\n","        return normalized_action\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.560413Z","iopub.status.busy":"2024-12-03T16:44:16.559937Z","iopub.status.idle":"2024-12-03T16:44:16.578267Z","shell.execute_reply":"2024-12-03T16:44:16.576882Z","shell.execute_reply.started":"2024-12-03T16:44:16.560363Z"},"trusted":true},"outputs":[],"source":["# Configuration of the network\n","with open('/kaggle/input/config-v0/network_config_v1.json') as file:\n","    network_config = file.read()\n","\n","EP_LENGTH = 100 # Length of the episode"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.580652Z","iopub.status.busy":"2024-12-03T16:44:16.579921Z","iopub.status.idle":"2024-12-03T16:44:16.617458Z","shell.execute_reply":"2024-12-03T16:44:16.616365Z","shell.execute_reply.started":"2024-12-03T16:44:16.580597Z"},"trusted":true},"outputs":[],"source":["def load_config(config_file):\n","\n","    with open(config_file, \"r\") as f:\n","        config = json.load(f)\n","    return config\n","\n","def make_env(config_file=\"config.json\"):\n","\n","    config = load_config(config_file)\n","\n","    env = SS_Mngmt_Env(\n","        network_config=network_config,\n","        EP_LENGTH=EP_LENGTH,\n","        render_mode=\"human\",\n","        model_type=\"DDPG\",\n","        stockout_cost=config[\"stockout_cost\"],\n","        order_cost=config[\"order_cost\"],\n","        item_cost=config[\"item_cost\"],\n","        stock_cost=config[\"stock_cost\"],\n","        item_prize=config[\"item_prize\"],\n","        progressive_stock_cost=config[\"progressive_stock_cost\"],\n","        stock_out_max=config[\"stock_out_max\"],\n","        order_quantities=config[\"order_quantities\"],\n","        demand_mean=config[\"demand_mean\"],\n","        demand_std=config[\"demand_std\"],\n","        demand_noise=config[\"demand_noise\"],\n","        demand_noise_std=config[\"demand_noise_std\"],\n","        demand_prob=config[\"demand_prob\"],\n","    )\n","\n","    # Wrap the environment with the monitor and for box actions\n","    return Monitor(MultiDiscreteToBoxWrapper(env))\n","\n","env = make_env(\"/kaggle/input/config-v0/env_config_v0.json\")\n","check_env(env, warn=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.620566Z","iopub.status.busy":"2024-12-03T16:44:16.620080Z","iopub.status.idle":"2024-12-03T16:44:16.651050Z","shell.execute_reply":"2024-12-03T16:44:16.649966Z","shell.execute_reply.started":"2024-12-03T16:44:16.620515Z"},"trusted":true},"outputs":[],"source":["vec_env = DummyVecEnv([lambda: make_env(\"/kaggle/input/config-v0/env_config_v0.json\")])\n","vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n","\n","model_path = os.path.join('Training', 'Models', 'DDPG')\n","log_path = os.path.join('Training', 'Logs', 'DDPG')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_parameters = {\n","    'learning_rate': 0.001,\n","    'tau': 0.005,\n","    'gamma': 0.99,\n","    'buffer_size' : 1_000_000,\n","    'batch_size' : 256,\n","    'noise_std' : 0.1,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-03T16:44:16.653698Z","iopub.status.busy":"2024-12-03T16:44:16.653184Z"},"trusted":true},"outputs":[],"source":["def optimize_hyperparams(trial):\n","    # Suggest the most important hyperparameters\n","    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n","    tau = trial.suggest_float(\"tau\", 1e-3, 1e-1, log=True)\n","    gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n","    buffer_size = trial.suggest_int(\"buffer_size\", 50000, 500000, step=50000)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n","    noise_std = trial.suggest_float(\"noise_std\", 0.1, 0.5)\n","\n","    # Define action noise\n","    n_actions = env.action_space.shape[0]\n","    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions))\n","\n","    # Create the DDPG model with static and optimized parameters\n","    model = DDPG(\n","        \"MultiInputPolicy\",\n","        vec_env,\n","        verbose=0,\n","        learning_rate=lr,\n","        buffer_size=buffer_size,\n","        batch_size=batch_size,\n","        tau=tau,\n","        gamma=gamma,\n","        train_freq=(1, \"episode\"),\n","        gradient_steps=1,\n","        learning_starts=100,\n","        tensorboard_log=log_path,\n","        action_noise=action_noise,\n","    )\n","\n","    # Train the model\n","    model.learn(total_timesteps=300_000)\n","\n","    # Evaluate the model\n","    rewards, _ = evaluate_policy(model, env, n_eval_episodes=5, return_episode_rewards=True)\n","\n","    # Return the average reward as the objective to maximize\n","    return sum(rewards) / len(rewards)\n","\n","# Run the hyperparameter optimization\n","study = optuna.create_study(direction=\"maximize\")\n","study.enqueue_trial(baseline_parameters)\n","study.optimize(optimize_hyperparams, n_trials=25)\n","\n","# Print the best parameters\n","print(\"Best parameters:\", study.best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["now = datetime.now()\n","\n","# Save the study to a file\n","with open(f\"/kaggle/working/DDPG_optuna_study_{now.strftime('%Y-%m-%d_%H_%M')}.pkl\", \"wb\") as f:\n","    pickle.dump(study, f)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6207537,"sourceId":10071183,"sourceType":"datasetVersion"}],"dockerImageVersionId":30804,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
