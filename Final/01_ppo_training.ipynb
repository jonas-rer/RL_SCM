{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10071183,"sourceType":"datasetVersion","datasetId":6207537}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n\n# Gymnasium imports\n\nimport gymnasium as gym\n\nfrom gymnasium import Env\n\nfrom gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n\n\n\nimport networkx as nx\n\nfrom networkx.drawing.nx_agraph import graphviz_layout\n\n\n\n# Import helpers\n\nimport numpy as np\n\nimport pandas as pd\n\nimport random\n\nimport os\n\nimport json\n\nimport optuna\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom itertools import product\n\nfrom pathlib import Path\n\nfrom datetime import datetime\n\nimport pickle\n\nimport pprint\n\n\n\nfrom collections import deque\n\n\n\n# Import stable baselines\n\nfrom stable_baselines3 import PPO, A2C\n\nfrom stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\n\nfrom stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\n\nfrom stable_baselines3.common.monitor import Monitor\n\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nfrom stable_baselines3.common.env_checker import check_env","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration of the network\n\nwith open(\"/kaggle/input/config-v0/network_config_v1.json\") as file:\n\n    network_config = file.read()\n\n\n\nEP_LENGTH = 100  # Length of the episode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SS_Mngmt_Env(Env):\n\n    \"\"\"\n\n    Supply Chain Management Environment\n\n    Environment for supply chain management with a single product and multiple nodes.\n\n    The action space constists of the order quantities for each node.\n\n    The observation space consists of the inventory levels and the planned and actual demand for each node.\n\n    \"\"\"\n\n\n\n    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n\n\n\n    # Define the action and observation space\n\n    def __init__(\n\n        self,\n\n        EP_LENGTH=52,\n\n        network_config=None,\n\n        render_mode=None,\n\n        model_type=None,\n\n        stockout_cost=500,  # Cost of stockout\n\n        stock_out_max=9,  # Maximum number of stockouts\n\n        order_cost=5,  # Cost of each order\n\n        item_cost=0.1,  # Cost of each item\n\n        stock_cost=0.5,  # Cost of stock per unit\n\n        item_prize=20,  # Prize of each item\n\n        order_quantities=[0, 15, 50],  # Order quantities for each node\n\n        demand_mean=10,  # Mean demand\n\n        demand_std=2,  # Standard deviation of demand\n\n        demand_noise=0,  # Mean noise in demand\n\n        demand_noise_std=2,  # Standard deviation of noise in demand\n\n        demand_prob=0.4,  # Probability of having demand\n\n        intermediate_reward=1000,  # Intermediate reward\n\n        progressive_stock_cost=False,  # Progressive stock cost\n\n        kaggle=False,  # Kaggle mode (True or False)\n\n        extreme=False,  # Extreme mode (True or False)\n\n        seasonality=False,  # Seasonality mode (True or False)\n\n    ):\n\n        \"\"\"\n\n        Initialize the environment\n\n        EP_LENGTH: int - Total length of the episode\n\n        network_config: str - JSON string with network configuration\n\n        render_mode: str - Render mode for the environment\n\n        model_type: str - Type of model (e.g., PPO, A2C)\n\n        stockout_cost: float - Cost of stockout\n\n        stock_out_max: int - Maximum number of stockouts\n\n        order_cost: float - Cost of each order\n\n        item_cost: float - Cost of each item\n\n        stock_cost: float - Cost of stock per unit\n\n        item_prize: float - Prize of each item\n\n        order_quantities: list - Order quantities for each node\n\n        demand_mean: float - Mean demand\n\n        demand_std: float - Standard deviation of demand\n\n        demand_noise: float - Mean noise in demand\n\n        demand_noise_std: float - Standard deviation of noise in demand\n\n        demand_prob: float - Probability of having demand\n\n        progressive_stock_cost: bool - Progressive stock cost\n\n        kaggle: bool - Kaggle mode (True or False)\n\n        \"\"\"\n\n\n\n        self.EP_LENGTH = EP_LENGTH  # Total length\n\n        self.episode_length = EP_LENGTH  # Current length of the episode\n\n        self.intermediate_reward = intermediate_reward  # Intermediate reward\n\n\n\n        self.total_reward = 0\n\n\n\n        self.model_type = model_type\n\n\n\n        # Set the data path\n\n        now = datetime.now()\n\n        self.data_path = (\n\n            f'./Data/{now.strftime(\"%Y-%m-%d\")}_environment_data_{self.model_type}.csv'\n\n        )\n\n\n\n        # Set the fieldnames for the CSV file\n\n        self.fieldnames = [\n\n            \"Time\",\n\n            \"Node\",\n\n            \"Stock\",\n\n            \"Action\",\n\n            \"Demand\",\n\n            \"Delivery\",\n\n            \"Reward\",\n\n            \"Total Reward\",\n\n            \"Backlog\",\n\n        ]\n\n\n\n        self.file_initialized = False\n\n\n\n        # Seting up the network\n\n        self.network_config = network_config\n\n        self.graph = nx.DiGraph()\n\n        self.setup_network(self.network_config)\n\n\n\n        self.lead_times = nx.get_edge_attributes(self.graph, \"L\")\n\n\n\n        # Number of nodes excluding 'S' and 'D'\n\n        num_nodes = len(self.graph.nodes) - 2\n\n\n\n        # Define the costs\n\n        self.stockout_cost = stockout_cost\n\n        self.order_cost = order_cost\n\n        self.item_cost = item_cost\n\n        self.stock_cost = stock_cost\n\n        self.stock_out_max = stock_out_max\n\n        self.item_prize = item_prize\n\n        self.progressive_stock_cost = progressive_stock_cost\n\n\n\n        self.stock_out_counter = 0\n\n\n\n        self.order_quantities = order_quantities\n\n\n\n        # Order delay and queue\n\n        self.order_queues = self.order_queue(initial_order=order_quantities[1])\n\n\n\n        # Backlog queue for each node\n\n        self.backlog_queues = self.backlog_queue()\n\n\n\n        # Define action space\n\n        n_actions = len(order_quantities)\n\n        n_nodes = len(self.graph.nodes) - 2\n\n        action_choices = np.full(n_nodes, n_actions)\n\n        self.action_space = MultiDiscrete(action_choices)\n\n\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n\n        self.observation_space = Dict(\n\n            {\n\n                \"inventory_levels\": Box(\n\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n\n                ),\n\n                \"current_demand\": Box(\n\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n\n                ),\n\n                \"backlog_levels\": Box(\n\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n\n                ),\n\n                \"order_queues\": Box(\n\n                    low=0, high=1000, shape=(num_nodes, max_lead_time), dtype=np.float32\n\n                ),\n\n                \"lead_times\": Box(\n\n                    low=1, high=max_lead_time, shape=(num_nodes,), dtype=np.int32\n\n                ),\n\n            }\n\n        )\n\n\n\n        # Setting up the initial state\n\n        self.demand_mean = demand_mean\n\n        self.demand_std = demand_std\n\n        self.demand_noise = demand_noise\n\n        self.demand_noise_std = demand_noise_std\n\n        self.demand_prob = demand_prob\n\n        self.extreme = extreme\n\n        self.seasonality = seasonality\n\n\n\n        if self.seasonality == True:\n\n\n\n            self.planned_demands = self.planned_demand_seasonality(\n\n                self.demand_mean, self.demand_std, self.demand_prob\n\n            )\n\n\n\n        else:\n\n\n\n            self.planned_demands = self.planned_demand(\n\n                self.demand_mean, self.demand_std, self.demand_prob\n\n            )\n\n\n\n        if self.extreme == True:\n\n\n\n            self.actual_demands = self.actual_demand_extremes(\n\n                self.planned_demands, self.demand_noise_std, self.demand_noise\n\n            )\n\n\n\n        else:\n\n\n\n            self.actual_demands = self.actual_demand(\n\n                self.planned_demands, self.demand_noise_std, self.demand_noise\n\n            )\n\n\n\n        # Collect initial inventories from the graph\n\n        initial_inventories = []\n\n        for node in self.graph.nodes:\n\n            if node not in [\"S\", \"D\"]:\n\n                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n\n\n\n        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n\n\n\n        self.state = {\n\n            \"inventory_levels\": initial_inventories.astype(np.float32),\n\n            \"planned_demand\": self.planned_demands,\n\n            \"actual_demand\": self.actual_demands,\n\n            \"current_demand\": self.actual_demands[0],\n\n            \"backlog_levels\": np.zeros(num_nodes),\n\n            \"order_queue_status\": np.zeros(num_nodes),\n\n        }\n\n\n\n        # Prep to save the data\n\n        self.inventory = initial_inventories\n\n        self.stock_history = [self.inventory.tolist()]\n\n        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n\n\n\n        # Kaggle mode\n\n        self.kaggle = kaggle\n\n\n\n        # Render mode\n\n        self.render_mode = render_mode\n\n        self.screen_initialized = False\n\n\n\n    # Defining the step function\n\n    def step(self, action):\n\n        \"\"\"\n\n        Executes one step in the environment.\n\n        Starts by processing the orders and updating the inventory levels for each node.\n\n        Then, it computes the reward based on the order costs and stock level.\n\n        Finally, it checks if the episode is done and returns the next state, reward, and whether the episode is done.\n\n        \"\"\"\n\n\n\n        # Returns the next state, reward and whether the episode is done\n\n        timestep = self.EP_LENGTH - self.episode_length\n\n\n\n        # num_nodes = len(self.graph.nodes) - 2\n\n\n\n        # Retrieve the current inventory levels\n\n        self.inventory = self.state[\"inventory_levels\"]\n\n        inventory_levels = np.copy(self.inventory)\n\n        reward = 0\n\n\n\n        # Retrieve the actual demand for the current timestep\n\n        self.current_demand = self.actual_demands[timestep].astype(np.float32)\n\n\n\n        # Add every first element of the order queues to the history\n\n        self.new_order = [self.order_quantities[i] for i in action]\n\n\n\n        # For visualization and history data\n\n        self.orders = np.array(\n\n            [\n\n                self.order_queues[node][0]\n\n                for node in self.graph.nodes\n\n                if node not in [\"S\", \"D\"]\n\n            ]\n\n        )\n\n\n\n        # Process the orders and update the inventory levels for each node\n\n        for node in self.graph.nodes:\n\n            if node not in [\"S\", \"D\"]:\n\n                node_index = self.node_to_index(node)\n\n\n\n                # Deduct costs for placing new orders\n\n                if self.new_order[node_index] > 0:\n\n                    reward -= self.order_cost + (\n\n                        self.new_order[node_index] * self.item_cost\n\n                    )\n\n\n\n                # Fulfill orders from the queue\n\n                order = self.order_queues[node].popleft()\n\n                inventory_levels[node_index] += order\n\n\n\n                # Attempt to meet current demand\n\n                node_demand = self.current_demand[node_index]\n\n                if inventory_levels[node_index] >= node_demand:\n\n                    # Enough stock to meet demand\n\n                    inventory_levels[node_index] -= node_demand\n\n                    reward += node_demand * self.item_prize\n\n                else:\n\n                    # Insufficient stock - add unmet demand to backlog and apply penalty\n\n                    unmet_demand = node_demand - inventory_levels[node_index]\n\n                    inventory_levels[node_index] -= node_demand - unmet_demand\n\n                    reward += (node_demand - unmet_demand) * self.item_prize\n\n                    reward -= self.stockout_cost * unmet_demand  # Apply stockout cost\n\n                    self.backlog_queues[node].append(unmet_demand)\n\n\n\n                    # Increment the stockout counter\n\n                    self.stock_out_counter += 1\n\n\n\n                # Process backlog with any remaining stock\n\n                while self.backlog_queues[node] and inventory_levels[node_index] > 0:\n\n                    backlog_demand = self.backlog_queues[node][0]\n\n                    if inventory_levels[node_index] >= backlog_demand:\n\n                        inventory_levels[node_index] -= backlog_demand\n\n                        reward += backlog_demand * self.item_prize\n\n                        self.backlog_queues[node].popleft()\n\n                    else:\n\n                        break  # Not enough stock to clear the backlog completely\n\n\n\n                # backlog penalty\n\n                reward -= self.stockout_cost * len(self.backlog_queues[node])\n\n\n\n                # Replenish order queue\n\n                self.order_queues[node].append(self.new_order[node_index])\n\n\n\n        if self.progressive_stock_cost == False:\n\n            # Compute the reward based on the order costs and stock level\n\n            reward -= np.sum(inventory_levels) * self.stock_cost\n\n        elif self.progressive_stock_cost == True:\n\n            reward -= np.sum(\n\n                [\n\n                    self.quadratic_stock_cost(self.stock_cost, inv)\n\n                    for inv in inventory_levels\n\n                ]\n\n            )\n\n\n\n        # Penalty if the episode cannot be completed\n\n        if self.stock_out_counter >= self.stock_out_max:\n\n            reward -= (\n\n                self.episode_length * self.stockout_cost * (len(self.graph.nodes) - 2)\n\n            )\n\n\n\n        # Intermediate reward\n\n        if timestep % 2 == 0:\n\n            reward += self.intermediate_reward\n\n\n\n        # Update the reward\n\n        self.total_reward += reward\n\n\n\n        # Decrease the episode length\n\n        self.episode_length -= 1\n\n\n\n        inventory_levels = inventory_levels.flatten()\n\n        self.inventory = inventory_levels\n\n\n\n        self.state = {\n\n            \"inventory_levels\": inventory_levels.astype(np.float32),\n\n            \"planned_demand\": self.planned_demands,\n\n            \"actual_demand\": self.actual_demands,\n\n            \"current_demand\": self.actual_demands[timestep],\n\n            \"backlog_levels\": self.backlog_queues,\n\n            \"order_queue_status\": self.order_queues,\n\n        }\n\n\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n\n\n\n        obs = {\n\n            \"inventory_levels\": self.inventory.astype(np.float32),\n\n            \"current_demand\": self.actual_demands[timestep].astype(np.float32),\n\n            \"backlog_levels\": np.array(\n\n                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n\n            ),\n\n            \"order_queues\": np.array(\n\n                [\n\n                    list(self.order_queues[node])\n\n                    + [0] * (max_lead_time - len(self.order_queues[node]))\n\n                    for node in self.graph.nodes\n\n                    if node not in [\"S\", \"D\"]\n\n                ],\n\n                dtype=np.float32,\n\n            ),\n\n            \"lead_times\": np.array(\n\n                [\n\n                    len(self.order_queues[node])\n\n                    for node in self.graph.nodes\n\n                    if node not in [\"S\", \"D\"]\n\n                ],\n\n                dtype=np.int32,\n\n            ),\n\n        }\n\n\n\n        # Update the history data\n\n        self.reward_history.append(reward)\n\n        self.stock_history.append(list(self.inventory))\n\n\n\n        # self.log_step_data(timestep, action, reward)\n\n\n\n        # Check if the episode is done\n\n        done = self.episode_length == 0\n\n\n\n        # Check if episode is done\n\n        if self.stock_out_counter >= self.stock_out_max:\n\n\n\n            done = True\n\n\n\n        elif self.episode_length <= 0:\n\n\n\n            done = True\n\n\n\n        else:\n\n\n\n            done = False\n\n\n\n        # Set placeholder for info\n\n        info = {}\n\n\n\n        # Check if the episode is truncated\n\n        truncated = False\n\n\n\n        return obs, float(reward), done, truncated, info\n\n\n\n    def quadratic_stock_cost(self, stock_cost, inventory_level):\n\n        \"\"\"\n\n        Quadratic stock cost function.\n\n        \"\"\"\n\n        return stock_cost * (inventory_level**2)  # Quadratic cost\n\n\n\n    def log_step_data(self, timestep, action, reward):\n\n\n\n        if not self.file_initialized:\n\n            with open(self.data_path, \"w\", newline=\"\") as csvfile:\n\n                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n\n                writer.writeheader()\n\n            self.file_initialized = True  # Mark as initialized\n\n\n\n        for n in range(len(self.inventory)):\n\n            node_name = self.get_node_name(n)\n\n            row = {\n\n                \"Time\": timestep + 1,\n\n                \"Node\": node_name,\n\n                \"Stock\": self.inventory[n],\n\n                \"Action\": self.new_order[n],\n\n                \"Demand\": self.current_demand[n],\n\n                \"Delivery\": self.orders[n],\n\n                \"Reward\": reward,\n\n                \"Total Reward\": self.total_reward,\n\n                \"Backlog\": len(self.backlog_queues[node_name]) > 0,\n\n            }\n\n\n\n            # Append the row to the CSV file\n\n            with open(self.data_path, \"a\", newline=\"\") as csvfile:\n\n                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n\n                writer.writerow(row)\n\n\n\n    def reward_function(self):\n\n        # TODO - Implement a custom reward function\n\n\n\n        return 0\n\n\n\n    def render(self):\n\n        # Just check episode lenghth and only plot the last one when using matplotlib\n\n        if self.render_mode is not None:\n\n            if self.render_mode == \"human\":\n\n                self.render_human()\n\n\n\n    def render_human(self):\n\n        \"\"\"\n\n        Renders the environment in human mode.\n\n        Useful for debugging and visualization.\n\n        \"\"\"\n\n\n\n        print(\"*\" * 50)\n\n        print(\"\\nEpisode Information\")\n\n        print(f\"Episode Length: {self.EP_LENGTH - self.episode_length}\")\n\n        if len(self.stock_history) > 1:\n\n            print(f\"Stock Level (Previous Timestep): {self.stock_history[-2]}\")\n\n        else:\n\n            print(\n\n                \"Stock Level (Previous Timestep): No previous timestep data available\"\n\n            )\n\n        print(f\"Stock Level: {self.inventory}\")\n\n        print(\n\n            f\"Planned Demand: {self.planned_demands[self.EP_LENGTH - self.episode_length - 1]}\"\n\n        )\n\n        print(f\"Actual Demand: {self.current_demand}\")\n\n        print(f\"Action: {self.new_order}\")\n\n        print(f\"Deliveries: {self.orders}\")\n\n        # print(\n\n        #     f\"Previous Reward: {self.reward_history[self.EP_LENGTH - self.episode_length - 1]}\"\n\n        # )\n\n        print(f\"Step Reward: {self.reward_history[-1]}\")\n\n        print(f\"Total Reward: {self.total_reward}\")\n\n\n\n        print(\"\\nBacklog:\")\n\n        print([len(queue) > 0 for queue in self.backlog_queues.values()])\n\n        # pprint(self.backlog_queues, indent=4)\n\n\n\n        print(\"\\nOrder Queue:\")\n\n        pprint(self.order_queues, indent=4)\n\n        print()\n\n\n\n        # print(\"Stockout Cost: \", self.stockout_cost)\n\n        print(\"\\nStockout Counter: \", self.stock_out_counter)\n\n\n\n        return\n\n\n\n    def setup_network(self, network_config=None):\n\n        \"\"\"\n\n        Sets up the network graph based on the configuration provided.\n\n        \"\"\"\n\n        config = json.loads(network_config)\n\n\n\n        # Add nodes to the graph\n\n        for node, attributes in config[\"nodes\"].items():\n\n            self.graph.add_node(node, **attributes)\n\n\n\n        # Add edges to the graph with lead times\n\n        for edge in config[\"edges\"]:\n\n            self.graph.add_edge(edge[\"source\"], edge[\"target\"], L=edge[\"L\"])\n\n\n\n    def render_network(self):\n\n        \"\"\"\n\n        Renders the network graph using NetworkX and Matplotlib.\n\n        \"\"\"\n\n\n\n        print(\"Node Attributes:\")\n\n        for node, attributes in self.graph.nodes(data=True):\n\n            print(f\"Node {node}: {attributes}\")\n\n\n\n        pos = graphviz_layout(self.graph, prog=\"dot\")\n\n\n\n        plt.figure(figsize=(8, 6))\n\n\n\n        nx.draw_networkx_nodes(self.graph, pos, node_size=700, node_color=\"lightblue\")\n\n        nx.draw_networkx_edges(\n\n            self.graph, pos, edgelist=self.graph.edges(), arrowstyle=\"->\", arrowsize=20\n\n        )\n\n        nx.draw_networkx_labels(self.graph, pos, font_size=12, font_family=\"sans-serif\")\n\n\n\n        edge_labels = nx.get_edge_attributes(self.graph, \"L\")\n\n        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n\n\n\n        plt.title(\"Supply Chain Network Graph\", fontsize=15)\n\n\n\n        # Display the plot\n\n        plt.axis(\"off\")\n\n        plt.show()\n\n\n\n    def node_to_index(self, node):\n\n        \"\"\"\n\n        Returns the index of the node given its name.\n\n        \"\"\"\n\n        return list(self.graph.nodes).index(node)\n\n\n\n    def get_node_name(self, index):\n\n        \"\"\"\n\n        Returns the name of the node given its index.\n\n        \"\"\"\n\n        return list(self.graph.nodes)[index]\n\n\n\n    def planned_demand(self, demand_mean=10, demand_std=2, demand_prob=0.8):\n\n        \"\"\"\n\n        Generates planned demand for each edge in the network over the whole episode.\n\n        \"\"\"\n\n\n\n        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n\n\n\n        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n\n\n\n        for i, edge in enumerate(edges_leading_to_D):\n\n            for j in range(self.EP_LENGTH):\n\n                # Introduce a probability of having demand\n\n                if np.random.rand() < demand_prob:\n\n                    planned_demand[j, i] = int(\n\n                        np.random.normal(demand_mean, demand_std)\n\n                    )\n\n\n\n        return planned_demand\n\n\n\n    import numpy as np\n\n\n\n    def planned_demand_seasonality(\n\n        self,\n\n        demand_mean=10,\n\n        demand_std=2,\n\n        demand_prob=0.8,\n\n        season_length=50,\n\n        seasonality_strength=0.8,\n\n    ):\n\n        \"\"\"\n\n        Generates planned demand for each edge in the network over the whole episode,\n\n        incorporating seasonality into the demand pattern.\n\n\n\n        Parameters:\n\n        - demand_mean: Mean of the demand.\n\n        - demand_std: Standard deviation of the demand.\n\n        - demand_prob: Probability of demand occurring.\n\n        - season_length: Length of the seasonal cycle.\n\n\n\n        Returns:\n\n        - planned_demand: 2D array of planned demand with seasonality.\n\n        \"\"\"\n\n\n\n        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n\n\n\n        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n\n\n\n        # Generate seasonality factors for the episode length\n\n        seasonality_factors = 1 + seasonality_strength * np.sin(\n\n            2 * np.pi * np.arange(self.EP_LENGTH) / season_length\n\n        )\n\n\n\n        for i, edge in enumerate(edges_leading_to_D):\n\n            for j in range(self.EP_LENGTH):\n\n                # Introduce a probability of having demand\n\n                if np.random.rand() < demand_prob:\n\n                    # Apply seasonality to the mean demand\n\n                    seasonal_mean = demand_mean * seasonality_factors[j]\n\n                    planned_demand[j, i] = int(\n\n                        np.random.normal(seasonal_mean, demand_std)\n\n                    )\n\n\n\n        return planned_demand\n\n\n\n    def planned_demand_even(self, demand_mean=10, demand_std=2):\n\n        \"\"\"\n\n        Generates planned demand for each edge in the network over the whole episode.\n\n        The demand is distributed evenly, occurring only at fixed intervals (e.g., every fifth timestep).\n\n        \"\"\"\n\n\n\n        # Get edges leading to \"D\"\n\n        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n\n\n\n        # Initialize demand array with zeros\n\n        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n\n\n\n        for i, edge in enumerate(edges_leading_to_D):\n\n            # Determine timesteps where demand occurs (e.g., every fifth timestep)\n\n            timesteps_with_demand = np.arange(0, self.EP_LENGTH, 5)\n\n\n\n            for j in timesteps_with_demand:\n\n                # Generate demand from a normal distribution\n\n                demand = max(\n\n                    0, np.random.normal(demand_mean, demand_std)\n\n                )  # Ensure non-negative demand\n\n                planned_demand[j, i] = int(demand)\n\n\n\n        return planned_demand\n\n\n\n    def actual_demand(self, planned_demand, demand_noise_std=2, demand_noise=2):\n\n        \"\"\"\n\n        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n\n        \"\"\"\n\n\n\n        actual_demand = np.copy(planned_demand)\n\n\n\n        for i in range(actual_demand.shape[0]):\n\n            for j in range(actual_demand.shape[1]):\n\n                # Add a small random noise to the planned demand\n\n                if planned_demand[i, j] > 0:\n\n                    noise = np.random.normal(demand_noise, demand_noise_std)\n\n                    # Ensure actual demand is not less than 0\n\n                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n\n\n\n        return actual_demand\n\n\n\n    def actual_demand_extremes(\n\n        self,\n\n        planned_demand,\n\n        demand_noise_std=2,\n\n        demand_noise=2,\n\n        n_multiplications=[3, 6],\n\n        size_multiplier=[3, 4],\n\n    ):\n\n        \"\"\"\n\n        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n\n        To create a more interesting scenario, the demand is multiplied by a random factor 3-5 times over the whole demand.\n\n        \"\"\"\n\n        actual_demand = np.copy(planned_demand)\n\n\n\n        # Add random noise to planned demand\n\n        for i in range(actual_demand.shape[0]):\n\n            for j in range(actual_demand.shape[1]):\n\n                if planned_demand[i, j] > 0:\n\n                    noise = np.random.normal(demand_noise, demand_noise_std)\n\n                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n\n\n\n        # Multiply demand by a random factor 3-5 times\n\n        num_multiplications = np.random.randint(\n\n            n_multiplications[0], n_multiplications[1]\n\n        )  # Randomly select between 3 and 5\n\n        for _ in range(num_multiplications):\n\n            # Select random indices\n\n            i = np.random.randint(0, actual_demand.shape[0])\n\n            j = np.random.randint(0, actual_demand.shape[1])\n\n\n\n            # Apply a random multiplier\n\n            if planned_demand[i, j] > 0:\n\n                multiplier = np.random.uniform(size_multiplier[0], size_multiplier[1])\n\n                actual_demand[i, j] = int(actual_demand[i, j] * multiplier)\n\n\n\n        return actual_demand\n\n\n\n    def order_queue(self, initial_order=10):\n\n        \"\"\"\n\n        Creates a dictionary for the order queues for each node in the network.\n\n        \"\"\"\n\n\n\n        order_queues = {}\n\n\n\n        for node in self.graph.nodes:\n\n\n\n            if node not in [\"S\", \"D\"]:\n\n                in_edges = list(self.graph.in_edges(node, data=True))\n\n\n\n                if in_edges:\n\n                    lead_time = in_edges[0][2][\"L\"]\n\n                    order_queues[node] = deque(\n\n                        [initial_order] + [0] * (lead_time - 1), maxlen=lead_time\n\n                    )\n\n\n\n        return order_queues\n\n\n\n    def backlog_queue(self):\n\n        \"\"\"\n\n        Creates a dictionary for the backlog queues for each node in the network.\n\n        \"\"\"\n\n\n\n        backlog_queues = {}\n\n\n\n        for node in self.graph.nodes:\n\n            if node not in [\"S\", \"D\"]:\n\n\n\n                in_edges = list(self.graph.in_edges(node, data=True))\n\n                if in_edges:\n\n                    backlog_queues[node] = deque()\n\n\n\n        return backlog_queues\n\n\n\n    def save_state(self):\n\n        \"\"\"\n\n        Saves the current state of the environment.\n\n        Used for greedy algorithm.\n\n        \"\"\"\n\n\n\n        return {\n\n            \"episode_length\": self.episode_length,\n\n            \"inventory\": np.copy(self.inventory),\n\n            \"total_reward\": self.total_reward,\n\n            \"state\": self.state,\n\n            \"order_queues\": {k: deque(v) for k, v in self.order_queues.items()},\n\n            \"backlog_queues\": {k: deque(v) for k, v in self.backlog_queues.items()},\n\n        }\n\n\n\n    def load_state(self, saved_state):\n\n        \"\"\"\n\n        Loads the state of the environment.\n\n        Used for greedy algorithm.\n\n        \"\"\"\n\n\n\n        self.episode_length = saved_state[\"episode_length\"]\n\n        self.inventory = saved_state[\"inventory\"]\n\n        self.total_reward = saved_state[\"total_reward\"]\n\n        self.state = saved_state[\"state\"]\n\n        self.order_queues = {\n\n            k: deque(v) for k, v in saved_state[\"order_queues\"].items()\n\n        }\n\n        self.backlog_queues = {\n\n            k: deque(v) for k, v in saved_state[\"backlog_queues\"].items()\n\n        }\n\n\n\n    def reset(self, seed=None, options=None):\n\n        \"\"\"\n\n        Resets the environment to the initial state.\n\n        \"\"\"\n\n        super().reset(seed=seed)  # Reset the seed\n\n        if seed is not None:\n\n            random.seed(seed)\n\n\n\n        # Reset the episode length\n\n        self.episode_length = self.EP_LENGTH\n\n\n\n        self.file_initialized = False\n\n\n\n        self.total_reward = 0\n\n\n\n        # Reset the network\n\n        self.graph = nx.DiGraph()\n\n        self.setup_network(self.network_config)\n\n\n\n        num_nodes = len(self.graph.nodes) - 2\n\n\n\n        # Order delay and backlog queue\n\n        self.order_queues = self.order_queue(initial_order=self.order_quantities[1])\n\n        self.backlog_queues = self.backlog_queue()\n\n\n\n        self.stock_out_counter = 0\n\n\n\n        # Define the initial state\n\n\n\n        if self.seasonality == True:\n\n\n\n            self.planned_demands = self.planned_demand_seasonality(\n\n                self.demand_mean, self.demand_std, self.demand_prob\n\n            )\n\n\n\n        else:\n\n\n\n            self.planned_demands = self.planned_demand(\n\n                self.demand_mean, self.demand_std, self.demand_prob\n\n            )\n\n\n\n        if self.extreme == True:\n\n\n\n            self.actual_demands = self.actual_demand_extremes(\n\n                self.planned_demands, self.demand_noise_std, self.demand_noise\n\n            ).astype(np.float32)\n\n\n\n        else:\n\n\n\n            self.actual_demands = self.actual_demand(\n\n                self.planned_demands, self.demand_noise_std, self.demand_noise\n\n            ).astype(np.float32)\n\n\n\n        self.current_demand = self.actual_demands[0].astype(np.float32)\n\n\n\n        # Collect initial inventories from the graph\n\n        initial_inventories = []\n\n        for node in self.graph.nodes:\n\n            if node not in [\"S\", \"D\"]:\n\n                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n\n\n\n        # Convert to numpy array\n\n        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n\n\n\n        self.state = {\n\n            \"inventory_levels\": initial_inventories,\n\n            \"planned_demand\": self.planned_demands,\n\n            \"actual_demand\": self.current_demand,\n\n        }\n\n\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n\n        obs = {\n\n            \"inventory_levels\": self.inventory.astype(np.float32),\n\n            \"current_demand\": self.actual_demands[0].astype(np.float32),\n\n            \"backlog_levels\": np.array(\n\n                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n\n            ),\n\n            \"order_queues\": np.array(\n\n                [\n\n                    list(self.order_queues[node])\n\n                    + [0] * (max_lead_time - len(self.order_queues[node]))\n\n                    for node in self.graph.nodes\n\n                    if node not in [\"S\", \"D\"]\n\n                ],\n\n                dtype=np.float32,\n\n            ),\n\n            \"lead_times\": np.array(\n\n                [\n\n                    len(self.order_queues[node])\n\n                    for node in self.graph.nodes\n\n                    if node not in [\"S\", \"D\"]\n\n                ],\n\n                dtype=np.int32,\n\n            ),\n\n        }\n\n\n\n        # Resetting history data\n\n        self.inventory = initial_inventories\n\n        self.stock_history = [self.inventory.tolist()]\n\n        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n\n\n\n        # Placeholder for info\n\n        info = {}\n\n\n\n        return obs, info\n\n\n\n\n\nclass MultiDiscreteToBoxWrapper(gym.ActionWrapper):\n\n    \"\"\"\n\n    Converts a MultiDiscrete action space to a Box action space.\n\n    \"\"\"\n\n\n\n    def __init__(self, env):\n\n        super().__init__(env)\n\n        assert isinstance(\n\n            env.action_space, MultiDiscrete\n\n        ), \"Environment must have a MultiDiscrete action space.\"\n\n        self.original_action_space = env.action_space\n\n        self.action_space = Box(\n\n            low=0.0,\n\n            high=1.0,\n\n            shape=self.original_action_space.nvec.shape,\n\n            dtype=np.float32,\n\n        )\n\n\n\n    def action(self, action):\n\n        # Convert continuous action (Box) back to discrete (MultiDiscrete)\n\n        discrete_action = np.round(\n\n            action * (self.original_action_space.nvec - 1)\n\n        ).astype(int)\n\n        return discrete_action\n\n\n\n    def reverse_action(self, action):\n\n        # Optionally: Map discrete actions back to normalized (Box)\n\n        normalized_action = action / (self.original_action_space.nvec - 1)\n\n        return normalized_action\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_config(config_file):\n\n\n\n    with open(config_file, \"r\") as f:\n\n        config = json.load(f)\n\n    return config\n\n\n\ndef make_env(config_file=\"config.json\"):\n\n\n\n    config = load_config(config_file)\n\n\n\n    env = SS_Mngmt_Env(\n\n        network_config=network_config,\n\n        EP_LENGTH=EP_LENGTH,\n\n        render_mode=\"human\",\n\n        model_type=\"PPO\",\n\n        stockout_cost=config[\"stockout_cost\"],\n\n        order_cost=config[\"order_cost\"],\n\n        item_cost=config[\"item_cost\"],\n\n        stock_cost=config[\"stock_cost\"],\n\n        item_prize=config[\"item_prize\"],\n\n        progressive_stock_cost=config[\"progressive_stock_cost\"],\n\n        stock_out_max=config[\"stock_out_max\"],\n\n        order_quantities=config[\"order_quantities\"],\n\n        demand_mean=config[\"demand_mean\"],\n\n        demand_std=config[\"demand_std\"],\n\n        demand_noise=config[\"demand_noise\"],\n\n        demand_noise_std=config[\"demand_noise_std\"],\n\n        demand_prob=config[\"demand_prob\"],\n\n    )\n\n\n\n    return Monitor(env)\n\n\n\nenv = make_env(\"/kaggle/input/config-v0/env_config_v0.json\")\n\ncheck_env(env, warn=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_envs = 4\n\nenv_config_path = \"/kaggle/input/config-v0/env_config_v0.json\"\n\n\n\nvec_env = SubprocVecEnv([lambda: make_env(env_config_path) for _ in range(num_envs)])\n\nvec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vec_env = DummyVecEnv([lambda: make_env(\"/kaggle/input/config-v0/env_config_v0.json\")])\n\n# vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_parameters = {\n\n    \"learning_rate\": 0.0003,\n\n    \"gamma\": 0.99,\n\n    \"ent_coef\": 0.01,\n\n    \"vf_coef\": 0.5,\n\n    \"n_steps\": 2048,\n\n    \"batch_size\": 64,\n\n    \"clip_range\": 0.15,\n\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_hyperparams(trial):\n\n    # Define the hyperparameter search space\n\n    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n\n    gamma = trial.suggest_categorical(\"gamma\",[ 0.95, 0.97, 0.99, 0.999])\n\n    ent_coef = trial.suggest_float(\"ent_coef\", 1e-4, 1e-1, log=True)\n\n    vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n\n    n_steps = trial.suggest_categorical(\"n_steps\",[ 128, 256, 512, 1024, 2048])\n\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128, 256])\n\n    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.4)    \n\n\n\n    # Create and train the PPO model\n\n    model = PPO(\n\n        \"MultiInputPolicy\",\n\n        vec_env,\n\n        learning_rate=lr,\n\n        gamma=gamma,\n\n        ent_coef=ent_coef,\n\n        vf_coef=vf_coef,\n\n        n_steps=n_steps,\n\n        batch_size=batch_size,\n\n        clip_range=clip_range,\n\n        verbose=0,\n\n    )\n\n    model.learn(total_timesteps=600_000)\n\n\n\n    # Evaluate the model and return the mean reward\n\n    rewards, _ = evaluate_policy(model, env, n_eval_episodes=5, return_episode_rewards=True)\n\n    return sum(rewards) / len(rewards)\n\n\n\n# Set up the Optuna sampler - to focus on high performing hyperparameters\n\nsampler = optuna.samplers.TPESampler(seed=42, multivariate=True)\n\n\n\n# Set up the Optuna study and optimize\n\nstudy = optuna.create_study(direction=\"maximize\") # pass the sampler as a second argument for focus on optimization\n\nstudy.enqueue_trial(baseline_parameters)\n\nstudy.optimize(optimize_hyperparams, n_trials=30)\n\n\n\n# Print the best hyperparameters\n\nprint(\"Best parameters:\", study.best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"now = datetime.now()\n\n\n\n# Save the study to a file\n\nwith open(f\"/kaggle/working/PPO_optuna_study_{now.strftime('%Y-%m-%d_%H_%M')}.pkl\", \"wb\") as f:\n\n    pickle.dump(study, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot Optimization History**\n\n\n\nThis plot shows the best trial value at each step of the optimization process.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_optimization_history\n\n\n\nfig = plot_optimization_history(study)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot Hyperparameter Importance**\n\n\n\nThis plot estimates the relative importance of each hyperparameter based on how they affect the objective function.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_param_importances\n\n\n\nfig = plot_param_importances(study)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Parallel Coordinate Plot**\n\n\n\nThis plot helps visualize the relationships between hyperparameters and the objective value. It’s useful for spotting trends or correlations.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_parallel_coordinate\n\n\n\nfig = plot_parallel_coordinate(study)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Slice Plot**\n\n\n\nThis plot visualizes the objective value across the range of each hyperparameter, helping understand how each parameter influences the outcome.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_slice\n\n\n\nfig = plot_slice(study)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Contour Plot**\n\n\n\nThis plot shows the relationship between two hyperparameters and their influence on the objective value.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_contour\n\n\n\nfig = plot_contour(study)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}