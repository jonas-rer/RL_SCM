{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10071183,"sourceType":"datasetVersion","datasetId":6207537}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n# Gymnasium imports\nimport gymnasium as gym\nfrom gymnasium import Env\nfrom gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n\nimport networkx as nx\nfrom networkx.drawing.nx_agraph import graphviz_layout\n\n# Import helpers\nimport numpy as np\nimport pandas as pd\nimport random\nimport os\nimport json\nimport optuna\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import product\nfrom pathlib import Path\nfrom datetime import datetime\nimport pickle\n\nfrom collections import deque\n\n# Import stable baselines\nfrom stable_baselines3 import PPO, A2C\nfrom stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.env_checker import check_env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:07.040086Z","iopub.execute_input":"2024-12-04T06:14:07.040595Z","iopub.status.idle":"2024-12-04T06:14:07.049541Z","shell.execute_reply.started":"2024-12-04T06:14:07.040555Z","shell.execute_reply":"2024-12-04T06:14:07.048054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration of the network\nwith open(\"/kaggle/input/config-v0/network_config_v1.json\") as file:\n    network_config = file.read()\n\nEP_LENGTH = 100  # Length of the episode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:07.052173Z","iopub.execute_input":"2024-12-04T06:14:07.052756Z","iopub.status.idle":"2024-12-04T06:14:07.078287Z","shell.execute_reply.started":"2024-12-04T06:14:07.052702Z","shell.execute_reply":"2024-12-04T06:14:07.076833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SS_Mngmt_Env(Env):\n    \"\"\"\n    Supply Chain Management Environment\n    Environment for supply chain management with a single product and multiple nodes.\n    The action space constists of the order quantities for each node.\n    The observation space consists of the inventory levels and the planned and actual demand for each node.\n    \"\"\"\n\n    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n\n    # Define the action and observation space\n    def __init__(\n        self,\n        EP_LENGTH=52,\n        network_config=None,\n        render_mode=None,\n        model_type=None,\n        stockout_cost=500,  # Cost of stockout\n        stock_out_max=9,  # Maximum number of stockouts\n        order_cost=5,  # Cost of each order\n        item_cost=0.1,  # Cost of each item\n        stock_cost=0.5,  # Cost of stock per unit\n        item_prize=20,  # Prize of each item\n        order_quantities=[0, 15, 50],  # Order quantities for each node\n        demand_mean=10,  # Mean demand\n        demand_std=2,  # Standard deviation of demand\n        demand_noise=0,  # Mean noise in demand\n        demand_noise_std=2,  # Standard deviation of noise in demand\n        demand_prob=0.4,  # Probability of having demand\n        progressive_stock_cost=False,\n    ):\n        \"\"\"\n        Initialize the environment\n        EP_LENGTH: int - Total length of the episode\n        network_config: str - JSON string with network configuration\n        render_mode: str - Render mode for the environment\n        model_type: str - Type of model (e.g., PPO, A2C)\n        stockout_cost: float - Cost of stockout\n        stock_out_max: int - Maximum number of stockouts\n        order_cost: float - Cost of each order\n        item_cost: float - Cost of each item\n        stock_cost: float - Cost of stock per unit\n        item_prize: float - Prize of each item\n        order_quantities: list - Order quantities for each node\n        demand_mean: float - Mean demand\n        demand_std: float - Standard deviation of demand\n        demand_noise: float - Mean noise in demand\n        demand_noise_std: float - Standard deviation of noise in demand\n        demand_prob: float - Probability of having demand\n        \"\"\"\n\n        self.EP_LENGTH = EP_LENGTH  # Total length\n        self.episode_length = EP_LENGTH  # Current length of the episode\n\n        self.total_reward = 0\n\n        self.model_type = model_type\n\n        # Seting up the network\n        self.network_config = network_config\n        self.graph = nx.DiGraph()\n        self.setup_network(self.network_config)\n\n        self.lead_times = nx.get_edge_attributes(self.graph, \"L\")\n\n        # Number of nodes excluding 'S' and 'D'\n        num_nodes = len(self.graph.nodes) - 2\n\n        # Define the costs\n        self.stockout_cost = stockout_cost\n        self.order_cost = order_cost\n        self.item_cost = item_cost\n        self.stock_cost = stock_cost\n        self.stock_out_max = stock_out_max\n        self.item_prize = item_prize\n        self.progressive_stock_cost = progressive_stock_cost\n\n        self.stock_out_counter = 0\n\n        self.order_quantities = order_quantities\n\n        # Order delay and queue\n        self.order_queues = self.order_queue(initial_order=order_quantities[1])\n\n        # Backlog queue for each node\n        self.backlog_queues = self.backlog_queue()\n\n        # Define action space\n        n_actions = len(order_quantities)\n        n_nodes = len(self.graph.nodes) - 2\n        action_choices = np.full(n_nodes, n_actions)\n        self.action_space = MultiDiscrete(action_choices)\n\n        # Define the observation space\n        # self.observation_space = Dict(\n        #     {\n        #         \"inventory_levels\": Box(\n        #             low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n        #         ),\n        #         # \"planned_demand\": Box(\n        #         #     low=0, high=30, shape=(self.EP_LENGTH, num_nodes), dtype=np.float32\n        #         # ),\n        #         # \"actual_demand\": Box(\n        #         #     low=0, high=30, shape=(num_nodes,), dtype=np.float32\n        #         # ),\n        #     }\n        # )\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n        self.observation_space = Dict(\n            {\n                \"inventory_levels\": Box(\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n                ),\n                \"current_demand\": Box(\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n                ),\n                \"backlog_levels\": Box(\n                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n                ),\n                \"order_queues\": Box(\n                    low=0, high=1000, shape=(num_nodes, max_lead_time), dtype=np.float32\n                ),\n                \"lead_times\": Box(\n                    low=1, high=max_lead_time, shape=(num_nodes,), dtype=np.int32\n                ),\n            }\n        )\n\n        # self.observation_space = Dict(\n        #     {\n        #         \"inventory_levels\": Box(\n        #             low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n        #         ),\n        #         \"current_demand\": Box(\n        #             low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n        #         ),\n        #         \"backlog_levels\": Box(\n        #             low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n        #         ),\n        #         \"order_queue_status\": Box(\n        #             low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n        #         ),\n        #     }\n        # )\n\n        # Setting up the initial state\n        self.demand_mean = demand_mean\n        self.demand_std = demand_std\n        self.demand_noise = demand_noise\n        self.demand_noise_std = demand_noise_std\n        self.demand_prob = demand_prob\n\n        self.planned_demands = self.planned_demand(\n            self.demand_mean, self.demand_std, self.demand_prob\n        )\n        self.actual_demands = self.actual_demand(\n            self.planned_demands, self.demand_noise_std, self.demand_noise\n        )\n\n        # Collect initial inventories from the graph\n        initial_inventories = []\n        for node in self.graph.nodes:\n            if node not in [\"S\", \"D\"]:\n                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n\n        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n        # initial_inventories = initial_inventories.reshape(\n        #     1, initial_inventories.shape[0]\n        # )\n\n        self.state = {\n            \"inventory_levels\": initial_inventories.astype(np.float32),\n            \"planned_demand\": self.planned_demands,\n            \"actual_demand\": self.actual_demands,\n            \"current_demand\": self.actual_demands[0],\n            \"backlog_levels\": np.zeros(num_nodes),\n            \"order_queue_status\": np.zeros(num_nodes),\n        }\n\n        # Prep to save the data\n        self.inventory = initial_inventories\n        self.stock_history = [self.inventory.tolist()]\n        self.action_history = [np.zeros(num_nodes)]\n        self.demand_history = [np.zeros(num_nodes)]\n        self.delivery_history = [np.zeros(num_nodes)]\n        self.backlog_history = [[False, False, False]]\n        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n        self.total_reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n\n        # Render mode\n        self.render_mode = render_mode\n        self.screen_initialized = False\n\n    # Defining the step function\n    def step(self, action):\n        \"\"\"\n        Executes one step in the environment.\n        Starts by processing the orders and updating the inventory levels for each node.\n        Then, it computes the reward based on the order costs and stock level.\n        Finally, it checks if the episode is done and returns the next state, reward, and whether the episode is done.\n        \"\"\"\n\n        # Returns the next state, reward and whether the episode is done\n        timestep = self.EP_LENGTH - self.episode_length\n\n        # num_nodes = len(self.graph.nodes) - 2\n\n        # Retrieve the current inventory levels\n        self.inventory = self.state[\"inventory_levels\"]\n        inventory_levels = np.copy(self.inventory)\n        reward = 0\n\n        # Retrieve the actual demand for the current timestep\n        self.current_demand = self.actual_demands[timestep].astype(np.float32)\n\n        # Add every first element of the order queues to the history\n        self.new_order = [self.order_quantities[i] for i in action]\n\n        # For visualization and history data\n        self.orders = np.array(\n            [\n                self.order_queues[node][0]\n                for node in self.graph.nodes\n                if node not in [\"S\", \"D\"]\n            ]\n        )\n\n        # Process the orders and update the inventory levels for each node\n        for node in self.graph.nodes:\n            if node not in [\"S\", \"D\"]:\n                node_index = self.node_to_index(node)\n\n                # Deduct costs for placing new orders\n                if self.new_order[node_index] > 0:\n                    reward -= self.order_cost + (\n                        self.new_order[node_index] * self.item_cost\n                    )\n\n                # Fulfill orders from the queue\n                order = self.order_queues[node].popleft()\n                inventory_levels[node_index] += order\n\n                # Attempt to meet current demand\n                node_demand = self.current_demand[node_index]\n                if inventory_levels[node_index] >= node_demand:\n                    # Enough stock to meet demand\n                    inventory_levels[node_index] -= node_demand\n                    reward += node_demand * self.item_prize\n                else:\n                    # Insufficient stock - add unmet demand to backlog and apply penalty\n                    unmet_demand = node_demand - inventory_levels[node_index]\n                    inventory_levels[node_index] -= node_demand - unmet_demand\n                    reward += (node_demand - unmet_demand) * self.item_prize\n                    reward -= self.stockout_cost * unmet_demand  # Apply stockout cost\n                    self.backlog_queues[node].append(unmet_demand)\n\n                    # Increment the stockout counter\n                    self.stock_out_counter += 1\n\n                # Process backlog with any remaining stock\n                while self.backlog_queues[node] and inventory_levels[node_index] > 0:\n                    backlog_demand = self.backlog_queues[node][0]\n                    if inventory_levels[node_index] >= backlog_demand:\n                        inventory_levels[node_index] -= backlog_demand\n                        reward += backlog_demand * self.item_prize\n                        self.backlog_queues[node].popleft()\n                    else:\n                        break  # Not enough stock to clear the backlog completely\n\n                # backlog penalty\n                reward -= self.stockout_cost * len(self.backlog_queues[node])\n\n                # Replenish order queue\n                self.order_queues[node].append(self.new_order[node_index])\n\n        if self.progressive_stock_cost == False:\n            # Compute the reward based on the order costs and stock level\n            reward -= np.sum(inventory_levels) * self.stock_cost\n        elif self.progressive_stock_cost == True:\n            reward -= np.sum(\n                [\n                    self.quadratic_stock_cost(self.stock_cost, inv)\n                    for inv in inventory_levels\n                ]\n            )\n\n        # Penalty if the episode cannot be completed\n        if self.stock_out_counter >= self.stock_out_max:\n            reward -= (\n                self.episode_length * self.stockout_cost * (len(self.graph.nodes) - 2)\n            )\n\n        # Update the reward\n        self.total_reward += reward\n\n        # Decrease the episode length\n        self.episode_length -= 1\n\n        inventory_levels = inventory_levels.flatten()\n        self.inventory = inventory_levels\n\n        # Update the state\n        # self.state = {\n        #     \"inventory_levels\": inventory_levels.astype(np.float32),\n        #     \"planned_demand\": self.planned_demands,\n        #     \"actual_demand\": self.current_demand,\n        # }\n\n        self.state = {\n            \"inventory_levels\": inventory_levels.astype(np.float32),\n            \"planned_demand\": self.planned_demands,\n            \"actual_demand\": self.actual_demands,\n            \"current_demand\": self.actual_demands[timestep],\n            \"backlog_levels\": self.backlog_queues,\n            \"order_queue_status\": self.order_queues,\n        }\n\n        # Update the observation space\n        # obs = {\n        #     \"inventory_levels\": self.inventory.astype(np.float32),\n        #     # \"planned_demand\": self.planned_demands,\n        #     # \"actual_demand\": self.current_demand,\n        # }\n\n        # obs = {\n        #     \"inventory_levels\": self.inventory.astype(np.float32),\n        #     \"current_demand\": self.actual_demands[timestep].astype(np.float32),\n        #     \"backlog_levels\": np.array(\n        #         [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n        #     ),\n        #     \"order_queue_status\": np.array(\n        #         [\n        #             sum(self.order_queues[node])\n        #             for node in self.graph.nodes\n        #             if node not in [\"S\", \"D\"]\n        #         ],\n        #         dtype=np.float32,\n        #     ),\n        # }\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n        obs = {\n            \"inventory_levels\": self.inventory.astype(np.float32),\n            \"current_demand\": self.actual_demands[timestep].astype(np.float32),\n            \"backlog_levels\": np.array(\n                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n            ),\n            \"order_queues\": np.array(\n                [\n                    list(self.order_queues[node])\n                    + [0] * (max_lead_time - len(self.order_queues[node]))\n                    for node in self.graph.nodes\n                    if node not in [\"S\", \"D\"]\n                ],\n                dtype=np.float32,\n            ),\n            \"lead_times\": np.array(\n                [\n                    len(self.order_queues[node])\n                    for node in self.graph.nodes\n                    if node not in [\"S\", \"D\"]\n                ],\n                dtype=np.int32,\n            ),\n        }\n\n        # Update the history data\n        self.reward_history.append(reward)\n        self.total_reward_history.append(self.total_reward)\n        self.stock_history.append(list(self.inventory))\n        self.demand_history.append(self.current_demand)\n        self.action_history.append(self.new_order)\n        self.delivery_history.append(self.orders)\n        self.backlog_history.append(\n            [len(queue) > 0 for queue in self.backlog_queues.values()]\n        )\n\n        # Check if the episode is done\n        done = self.episode_length == 0\n\n        # Check if episode is done\n        if self.stock_out_counter >= self.stock_out_max:\n\n            # Save the data\n            now = datetime.now()\n            path = f'/kaggle/working/{now.strftime(\"%Y-%m-%d\")}_last_environment_data_{self.model_type}.csv'\n            self.save_data(path)\n\n            done = True\n\n        elif self.episode_length <= 0:\n\n            # Save the data\n            now = datetime.now()\n            path = f'/kaggle/working/{now.strftime(\"%Y-%m-%d\")}_last_environment_data_{self.model_type}.csv'\n            self.save_data(path)\n\n            done = True\n\n        else:\n\n            done = False\n\n        # Set placeholder for info\n        info = {}\n\n        # Check if the episode is truncated\n        truncated = False\n\n        return obs, float(reward), done, truncated, info\n\n    def quadratic_stock_cost(self, stock_cost, inventory_level):\n        \"\"\"\n        Quadratic stock cost function.\n        \"\"\"\n        return stock_cost * (inventory_level**2)  # Quadratic cost\n\n    def reward_function(self):\n        # TODO - Implement a custom reward function\n\n        return 0\n\n    def render(self):\n        # Just check episode lenghth and only plot the last one when using matplotlib\n        if self.render_mode is not None:\n            if self.render_mode == \"human\":\n                self.render_human()\n\n    def render_human(self):\n        \"\"\"\n        Renders the environment in human mode.\n        \"\"\"\n\n        print(\"*\" * 50)\n        print(\"\\nEpisode Information\")\n        print(f\"Episode Length: {self.EP_LENGTH - self.episode_length}\")\n        if len(self.stock_history) > 1:\n            print(f\"Stock Level (Previous Timestep): {self.stock_history[-2]}\")\n        else:\n            print(\n                \"Stock Level (Previous Timestep): No previous timestep data available\"\n            )\n        print(f\"Stock Level: {self.inventory}\")\n        print(\n            f\"Planned Demand: {self.planned_demands[self.EP_LENGTH - self.episode_length - 1]}\"\n        )\n        print(f\"Actual Demand: {self.current_demand}\")\n        print(f\"Action: {self.new_order}\")\n        print(f\"Deliveries: {self.orders}\")\n        # print(\n        #     f\"Previous Reward: {self.reward_history[self.EP_LENGTH - self.episode_length - 1]}\"\n        # )\n        print(f\"Step Reward: {self.reward_history[-1]}\")\n        print(f\"Total Reward: {self.total_reward}\")\n\n        print(\"\\nBacklog:\")\n        print([len(queue) > 0 for queue in self.backlog_queues.values()])\n        # pprint(self.backlog_queues, indent=4)\n\n        print(\"\\nOrder Queue:\")\n        pprint(self.order_queues, indent=4)\n        print()\n\n        # print(\"Stockout Cost: \", self.stockout_cost)\n        print(\"\\nStockout Counter: \", self.stock_out_counter)\n\n        return\n\n    def save_data(self, path):\n        \"\"\"\n        Saves the history data to a CSV file.\n        \"\"\"\n\n        # print(\n        #     f\"Lengths - stock: {len(self.stock_history)}, action: {len(self.action_history)}, \"\n        #     f\"demand: {len(self.demand_history)}, delivery: {len(self.delivery_history)}, \"\n        #     f\"reward: {len(self.reward_history)}, backlog: {len(self.backlog_history)}\"\n        # )\n\n        data = []\n\n        for t in range(len(self.stock_history)):\n            for n in range(len(self.stock_history[t])):\n                row = {\n                    \"Time\": t + 1,\n                    \"Node\": self.get_node_name(n),\n                    \"Stock\": self.stock_history[t][n],\n                    \"Action\": self.action_history[t][n],\n                    \"Demand\": self.demand_history[t][n],\n                    \"Delivery\": self.delivery_history[t][n],\n                    \"Reward\": self.reward_history[t],\n                    \"Total Reward\": self.total_reward_history[t],\n                    \"Backlog\": self.backlog_history[t][n],\n                }\n                data.append(row)\n\n        df = pd.DataFrame(data)\n\n        df.to_csv(path, index=False)\n\n        # print(f\"Data saved to {path}\")\n\n    def setup_network(self, network_config=None):\n        \"\"\"\n        Sets up the network graph based on the configuration provided.\n        \"\"\"\n        config = json.loads(network_config)\n\n        # Add nodes to the graph\n        for node, attributes in config[\"nodes\"].items():\n            self.graph.add_node(node, **attributes)\n\n        # Add edges to the graph with lead times\n        for edge in config[\"edges\"]:\n            self.graph.add_edge(edge[\"source\"], edge[\"target\"], L=edge[\"L\"])\n\n    def render_network(self):\n        \"\"\"\n        Renders the network graph using NetworkX and Matplotlib.\n        \"\"\"\n\n        print(\"Node Attributes:\")\n        for node, attributes in self.graph.nodes(data=True):\n            print(f\"Node {node}: {attributes}\")\n\n        pos = graphviz_layout(self.graph, prog=\"dot\")\n\n        plt.figure(figsize=(8, 6))\n\n        nx.draw_networkx_nodes(self.graph, pos, node_size=700, node_color=\"lightblue\")\n        nx.draw_networkx_edges(\n            self.graph, pos, edgelist=self.graph.edges(), arrowstyle=\"->\", arrowsize=20\n        )\n        nx.draw_networkx_labels(self.graph, pos, font_size=12, font_family=\"sans-serif\")\n\n        edge_labels = nx.get_edge_attributes(self.graph, \"L\")\n        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n\n        plt.title(\"Supply Chain Network Graph\", fontsize=15)\n\n        # Display the plot\n        plt.axis(\"off\")\n        plt.show()\n\n    def node_to_index(self, node):\n        \"\"\"\n        Returns the index of the node given its name.\n        \"\"\"\n        return list(self.graph.nodes).index(node)\n\n    def get_node_name(self, index):\n        \"\"\"\n        Returns the name of the node given its index.\n        \"\"\"\n        return list(self.graph.nodes)[index]\n\n    def planned_demand(self, demand_mean=10, demand_std=2, demand_prob=0.8):\n        \"\"\"\n        Generates planned demand for each edge in the network over the whole episode.\n        \"\"\"\n        # Generates a random planned demand for each edge in the network\n        # over the whole episode. The demand is drawn from a normal distribution\n\n        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n\n        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n\n        for i, edge in enumerate(edges_leading_to_D):\n            for j in range(self.EP_LENGTH):\n                # Introduce a probability of having demand\n                if np.random.rand() < demand_prob:\n                    planned_demand[j, i] = int(\n                        np.random.normal(demand_mean, demand_std)\n                    )\n\n        return planned_demand\n\n    def planned_demand_even(self, demand_mean, demand_std):\n        \"\"\"\n        Generates planned demand for each edge in the network over the whole episode.\n        The demand is distributed evenly, occurring only at fixed intervals (e.g., every fifth timestep).\n        \"\"\"\n\n        # Get edges leading to \"D\"\n        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n\n        # Initialize demand array with zeros\n        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n\n        for i, edge in enumerate(edges_leading_to_D):\n            # Determine timesteps where demand occurs (e.g., every fifth timestep)\n            timesteps_with_demand = np.arange(0, self.EP_LENGTH, 5)\n\n            for j in timesteps_with_demand:\n                # Generate demand from a normal distribution\n                demand = max(\n                    0, np.random.normal(demand_mean, demand_std)\n                )  # Ensure non-negative demand\n                planned_demand[j, i] = int(demand)\n\n        return planned_demand\n\n    def actual_demand(self, planned_demand, demand_noise_std, demand_noise):\n        \"\"\"\n        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n        \"\"\"\n\n        # Generate a random actual demand for each edge in the network\n        # based on the planned demand from the current timestep. The demand\n        # is drawn from a normal distribution\n        actual_demand = np.copy(planned_demand)\n\n        for i in range(actual_demand.shape[0]):\n            for j in range(actual_demand.shape[1]):\n                # Add a small random noise to the planned demand\n                if planned_demand[i, j] > 0:\n                    noise = np.random.normal(demand_noise, demand_noise_std)\n                    # Ensure actual demand is not less than 0\n                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n\n        return actual_demand\n\n    def order_queue(self, initial_order=10):\n        \"\"\"\n        Creates a dictionary for the order queues for each node in the network.\n        \"\"\"\n\n        order_queues = {}\n\n        for node in self.graph.nodes:\n\n            if node not in [\"S\", \"D\"]:\n                in_edges = list(self.graph.in_edges(node, data=True))\n\n                if in_edges:\n                    lead_time = in_edges[0][2][\"L\"]\n                    order_queues[node] = deque(\n                        [initial_order] + [0] * (lead_time - 1), maxlen=lead_time\n                    )\n\n        return order_queues\n\n    def backlog_queue(self):\n        \"\"\"\n        Creates a dictionary for the backlog queues for each node in the network.\n        \"\"\"\n\n        backlog_queues = {}\n\n        for node in self.graph.nodes:\n            if node not in [\"S\", \"D\"]:\n\n                in_edges = list(self.graph.in_edges(node, data=True))\n                if in_edges:\n                    backlog_queues[node] = deque()\n\n        return backlog_queues\n\n    def reset(self, seed=None):\n        \"\"\"\n        Resets the environment to the initial state.\n        \"\"\"\n        super().reset(seed=seed)  # Reset the seed\n        if seed is not None:\n            random.seed(seed)\n\n        # Reset the episode length\n        self.episode_length = self.EP_LENGTH\n\n        self.total_reward = 0\n\n        # Reset the network\n        self.graph = nx.DiGraph()\n        self.setup_network(self.network_config)\n\n        num_nodes = len(self.graph.nodes) - 2\n\n        # Order delay and backlog queue\n        self.order_queues = self.order_queue(initial_order=self.order_quantities[1])\n        self.backlog_queues = self.backlog_queue()\n\n        self.stock_out_counter = 0\n\n        # Define the initial state\n        self.planned_demands = self.planned_demand(\n            self.demand_mean, self.demand_std, self.demand_prob\n        ).astype(np.float32)\n\n        self.actual_demands = self.actual_demand(\n            self.planned_demands, self.demand_noise_std, self.demand_noise\n        ).astype(np.float32)\n\n        self.current_demand = self.actual_demands[0].astype(np.float32)\n\n        # Collect initial inventories from the graph\n        initial_inventories = []\n        for node in self.graph.nodes:\n            if node not in [\"S\", \"D\"]:\n                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n\n        # Convert to numpy array\n        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n\n        self.state = {\n            \"inventory_levels\": initial_inventories,\n            \"planned_demand\": self.planned_demands,\n            \"actual_demand\": self.current_demand,\n        }\n\n        # obs = {\n        #     \"inventory_levels\": initial_inventories,\n        #     # \"planned_demand\": self.planned_demands,\n        #     # \"actual_demand\": self.current_demand,\n        # }\n\n        # obs = {\n        #     \"inventory_levels\": self.inventory.astype(np.float32),\n        #     \"current_demand\": self.current_demand.astype(np.float32),\n        #     \"backlog_levels\": np.array(\n        #         [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n        #     ),\n        #     \"order_queue_status\": np.array(\n        #         [\n        #             sum(self.order_queues[node])\n        #             for node in self.graph.nodes\n        #             if node not in [\"S\", \"D\"]\n        #         ],\n        #         dtype=np.float32,\n        #     ),\n        # }\n\n        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n        obs = {\n            \"inventory_levels\": self.inventory.astype(np.float32),\n            \"current_demand\": self.actual_demands[0].astype(np.float32),\n            \"backlog_levels\": np.array(\n                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n            ),\n            \"order_queues\": np.array(\n                [\n                    list(self.order_queues[node])\n                    + [0] * (max_lead_time - len(self.order_queues[node]))\n                    for node in self.graph.nodes\n                    if node not in [\"S\", \"D\"]\n                ],\n                dtype=np.float32,\n            ),\n            \"lead_times\": np.array(\n                [\n                    len(self.order_queues[node])\n                    for node in self.graph.nodes\n                    if node not in [\"S\", \"D\"]\n                ],\n                dtype=np.int32,\n            ),\n        }\n\n        # Resetting history data\n        self.inventory = initial_inventories\n        self.stock_history = [self.inventory.tolist()]\n        self.action_history = [np.zeros(num_nodes)]\n        self.demand_history = [np.zeros(num_nodes)]\n        self.delivery_history = [np.zeros(num_nodes)]\n        self.backlog_history = [[False, False, False]]\n        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n        self.total_reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n\n        # Placeholder for info\n        info = {}\n\n        return obs, info\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:07.192279Z","iopub.execute_input":"2024-12-04T06:14:07.193108Z","iopub.status.idle":"2024-12-04T06:14:07.267954Z","shell.execute_reply.started":"2024-12-04T06:14:07.193063Z","shell.execute_reply":"2024-12-04T06:14:07.266406Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_config(config_file):\n\n    with open(config_file, \"r\") as f:\n        config = json.load(f)\n    return config\n\ndef make_env(config_file=\"config.json\"):\n\n    config = load_config(config_file)\n\n    env = SS_Mngmt_Env(\n        network_config=network_config,\n        EP_LENGTH=EP_LENGTH,\n        render_mode=\"human\",\n        model_type=\"PPO\",\n        stockout_cost=config[\"stockout_cost\"],\n        order_cost=config[\"order_cost\"],\n        item_cost=config[\"item_cost\"],\n        stock_cost=config[\"stock_cost\"],\n        item_prize=config[\"item_prize\"],\n        progressive_stock_cost=config[\"progressive_stock_cost\"],\n        stock_out_max=config[\"stock_out_max\"],\n        order_quantities=config[\"order_quantities\"],\n        demand_mean=config[\"demand_mean\"],\n        demand_std=config[\"demand_std\"],\n        demand_noise=config[\"demand_noise\"],\n        demand_noise_std=config[\"demand_noise_std\"],\n        demand_prob=config[\"demand_prob\"],\n    )\n\n    return Monitor(env)\n\nenv = make_env(\"/kaggle/input/config-v0/env_config_v0.json\")\ncheck_env(env, warn=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:07.271021Z","iopub.execute_input":"2024-12-04T06:14:07.271642Z","iopub.status.idle":"2024-12-04T06:14:07.301713Z","shell.execute_reply.started":"2024-12-04T06:14:07.271585Z","shell.execute_reply":"2024-12-04T06:14:07.300380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_envs = 4\nenv_config_path = \"/kaggle/input/config-v0/env_config_v0.json\"\n\nvec_env = SubprocVecEnv([lambda: make_env(env_config_path) for _ in range(num_envs)])\nvec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:07.303102Z","iopub.execute_input":"2024-12-04T06:14:07.303558Z","iopub.status.idle":"2024-12-04T06:14:18.819771Z","shell.execute_reply.started":"2024-12-04T06:14:07.303522Z","shell.execute_reply":"2024-12-04T06:14:18.816113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vec_env = DummyVecEnv([lambda: make_env(\"/kaggle/input/config-v0/env_config_v0.json\")])\n# vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:18.821206Z","iopub.execute_input":"2024-12-04T06:14:18.821722Z","iopub.status.idle":"2024-12-04T06:14:18.833491Z","shell.execute_reply.started":"2024-12-04T06:14:18.821667Z","shell.execute_reply":"2024-12-04T06:14:18.828893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_hyperparams(trial):\n    # Define the hyperparameter search space\n    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    gamma = trial.suggest_categorical(\"gamma\",[ 0.95, 0.97, 0.99, 0.999])\n    ent_coef = trial.suggest_float(\"ent_coef\", 1e-4, 1e-1, log=True)\n    vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n    n_steps = trial.suggest_categorical(\"n_steps\",[ 128, 256, 512, 1024])\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128, 256])\n    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.4)    \n\n    # Create and train the PPO model\n    model = PPO(\n        \"MultiInputPolicy\",\n        vec_env,\n        learning_rate=lr,\n        gamma=gamma,\n        ent_coef=ent_coef,\n        vf_coef=vf_coef,\n        n_steps=n_steps,\n        batch_size=batch_size,\n        clip_range=clip_range,\n        verbose=0,\n    )\n    model.learn(total_timesteps=600_000)\n\n    # Evaluate the model and return the mean reward\n    rewards, _ = evaluate_policy(model, env, n_eval_episodes=5, return_episode_rewards=True)\n    return sum(rewards) / len(rewards)\n\n# Set up the Optuna study and optimize\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(optimize_hyperparams, n_trials=30)\n\n# Print the best hyperparameters\nprint(\"Best parameters:\", study.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:14:18.838615Z","iopub.execute_input":"2024-12-04T06:14:18.839942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"now = datetime.now()\n\n# Save the study to a file\nwith open(f\"/kaggle/working/PPO_optuna_study_{now.strftime('%Y-%m-%d_%H_%M')}.pkl\", \"wb\") as f:\n    pickle.dump(study, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot Optimization History**\n\nThis plot shows the best trial value at each step of the optimization process.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_optimization_history\n\nfig = plot_optimization_history(study)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot Hyperparameter Importance**\n\nThis plot estimates the relative importance of each hyperparameter based on how they affect the objective function.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_param_importances\n\nfig = plot_param_importances(study)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Parallel Coordinate Plot**\n\nThis plot helps visualize the relationships between hyperparameters and the objective value. Itâ€™s useful for spotting trends or correlations.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_parallel_coordinate\n\nfig = plot_parallel_coordinate(study)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Slice Plot**\n\nThis plot visualizes the objective value across the range of each hyperparameter, helping understand how each parameter influences the outcome.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_slice\n\nfig = plot_slice(study)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Contour Plot**\n\nThis plot shows the relationship between two hyperparameters and their influence on the objective value.","metadata":{}},{"cell_type":"code","source":"from optuna.visualization import plot_contour\n\nfig = plot_contour(study)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}