{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be86d151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:34:33.892568Z",
     "iopub.status.busy": "2024-12-04T19:34:33.892237Z",
     "iopub.status.idle": "2024-12-04T19:34:53.559135Z",
     "shell.execute_reply": "2024-12-04T19:34:53.558225Z"
    },
    "papermill": {
     "duration": 19.674505,
     "end_time": "2024-12-04T19:34:53.561286",
     "exception": false,
     "start_time": "2024-12-04T19:34:33.886781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Gymnasium imports\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium import Env\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "\n",
    "# Import helpers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "# Import stable baselines\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecNormalize\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ad840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:34:53.570353Z",
     "iopub.status.busy": "2024-12-04T19:34:53.569393Z",
     "iopub.status.idle": "2024-12-04T19:34:53.631722Z",
     "shell.execute_reply": "2024-12-04T19:34:53.630902Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.068955,
     "end_time": "2024-12-04T19:34:53.633830",
     "exception": false,
     "start_time": "2024-12-04T19:34:53.564875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SS_Mngmt_Env(Env):\n",
    "    \"\"\"\n",
    "    Supply Chain Management Environment\n",
    "    Environment for supply chain management with a single product and multiple nodes.\n",
    "    The action space constists of the order quantities for each node.\n",
    "    The observation space consists of the inventory levels and the planned and actual demand for each node.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    # Define the action and observation space\n",
    "    def __init__(\n",
    "        self,\n",
    "        EP_LENGTH=52,\n",
    "        network_config=None,\n",
    "        render_mode=None,\n",
    "        model_type=None,\n",
    "        stockout_cost=500,  # Cost of stockout\n",
    "        stock_out_max=9,  # Maximum number of stockouts\n",
    "        order_cost=5,  # Cost of each order\n",
    "        item_cost=0.1,  # Cost of each item\n",
    "        stock_cost=0.5,  # Cost of stock per unit\n",
    "        item_prize=20,  # Prize of each item\n",
    "        order_quantities=[0, 15, 50],  # Order quantities for each node\n",
    "        demand_mean=10,  # Mean demand\n",
    "        demand_std=2,  # Standard deviation of demand\n",
    "        demand_noise=0,  # Mean noise in demand\n",
    "        demand_noise_std=2,  # Standard deviation of noise in demand\n",
    "        demand_prob=0.4,  # Probability of having demand\n",
    "        intermediate_reward=1000,  # Intermediate reward\n",
    "        progressive_stock_cost=False,  # Progressive stock cost\n",
    "        kaggle=False,  # Kaggle mode (True or False)\n",
    "        extreme=False,  # Extreme mode (True or False)\n",
    "        seasonality=False,  # Seasonality mode (True or False)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the environment\n",
    "        EP_LENGTH: int - Total length of the episode\n",
    "        network_config: str - JSON string with network configuration\n",
    "        render_mode: str - Render mode for the environment\n",
    "        model_type: str - Type of model (e.g., PPO, A2C)\n",
    "        stockout_cost: float - Cost of stockout\n",
    "        stock_out_max: int - Maximum number of stockouts\n",
    "        order_cost: float - Cost of each order\n",
    "        item_cost: float - Cost of each item\n",
    "        stock_cost: float - Cost of stock per unit\n",
    "        item_prize: float - Prize of each item\n",
    "        order_quantities: list - Order quantities for each node\n",
    "        demand_mean: float - Mean demand\n",
    "        demand_std: float - Standard deviation of demand\n",
    "        demand_noise: float - Mean noise in demand\n",
    "        demand_noise_std: float - Standard deviation of noise in demand\n",
    "        demand_prob: float - Probability of having demand\n",
    "        progressive_stock_cost: bool - Progressive stock cost\n",
    "        kaggle: bool - Kaggle mode (True or False)\n",
    "        \"\"\"\n",
    "\n",
    "        self.EP_LENGTH = EP_LENGTH  # Total length\n",
    "        self.episode_length = EP_LENGTH  # Current length of the episode\n",
    "        self.intermediate_reward = intermediate_reward  # Intermediate reward\n",
    "\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # Set the data path\n",
    "        now = datetime.now()\n",
    "        self.data_path = (\n",
    "            f'./Data/{now.strftime(\"%Y-%m-%d\")}_environment_data_{self.model_type}.csv'\n",
    "        )\n",
    "\n",
    "        # Set the fieldnames for the CSV file\n",
    "        self.fieldnames = [\n",
    "            \"Time\",\n",
    "            \"Node\",\n",
    "            \"Stock\",\n",
    "            \"Action\",\n",
    "            \"Demand\",\n",
    "            \"Delivery\",\n",
    "            \"Reward\",\n",
    "            \"Total Reward\",\n",
    "            \"Backlog\",\n",
    "        ]\n",
    "\n",
    "        self.file_initialized = False\n",
    "\n",
    "        # Seting up the network\n",
    "        self.network_config = network_config\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.setup_network(self.network_config)\n",
    "\n",
    "        self.lead_times = nx.get_edge_attributes(self.graph, \"L\")\n",
    "\n",
    "        # Number of nodes excluding 'S' and 'D'\n",
    "        num_nodes = len(self.graph.nodes) - 2\n",
    "\n",
    "        # Define the costs\n",
    "        self.stockout_cost = stockout_cost\n",
    "        self.order_cost = order_cost\n",
    "        self.item_cost = item_cost\n",
    "        self.stock_cost = stock_cost\n",
    "        self.stock_out_max = stock_out_max\n",
    "        self.item_prize = item_prize\n",
    "        self.progressive_stock_cost = progressive_stock_cost\n",
    "\n",
    "        self.stock_out_counter = 0\n",
    "\n",
    "        self.order_quantities = order_quantities\n",
    "\n",
    "        # Order delay and queue\n",
    "        self.order_queues = self.order_queue(initial_order=order_quantities[1])\n",
    "\n",
    "        # Backlog queue for each node\n",
    "        self.backlog_queues = self.backlog_queue()\n",
    "\n",
    "        # Define action space\n",
    "        n_actions = len(order_quantities)\n",
    "        n_nodes = len(self.graph.nodes) - 2\n",
    "        action_choices = np.full(n_nodes, n_actions)\n",
    "        self.action_space = MultiDiscrete(action_choices)\n",
    "\n",
    "        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"inventory_levels\": Box(\n",
    "                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n",
    "                ),\n",
    "                \"current_demand\": Box(\n",
    "                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n",
    "                ),\n",
    "                \"backlog_levels\": Box(\n",
    "                    low=0, high=1000, shape=(num_nodes,), dtype=np.float32\n",
    "                ),\n",
    "                \"order_queues\": Box(\n",
    "                    low=0, high=1000, shape=(num_nodes, max_lead_time), dtype=np.float32\n",
    "                ),\n",
    "                \"lead_times\": Box(\n",
    "                    low=1, high=max_lead_time, shape=(num_nodes,), dtype=np.int32\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Setting up the initial state\n",
    "        self.demand_mean = demand_mean\n",
    "        self.demand_std = demand_std\n",
    "        self.demand_noise = demand_noise\n",
    "        self.demand_noise_std = demand_noise_std\n",
    "        self.demand_prob = demand_prob\n",
    "        self.extreme = extreme\n",
    "        self.seasonality = seasonality\n",
    "\n",
    "        if self.seasonality == True:\n",
    "\n",
    "            self.planned_demands = self.planned_demand_seasonality(\n",
    "                self.demand_mean, self.demand_std, self.demand_prob\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.planned_demands = self.planned_demand(\n",
    "                self.demand_mean, self.demand_std, self.demand_prob\n",
    "            )\n",
    "\n",
    "        if self.extreme == True:\n",
    "\n",
    "            self.actual_demands = self.actual_demand_extremes(\n",
    "                self.planned_demands, self.demand_noise_std, self.demand_noise\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.actual_demands = self.actual_demand(\n",
    "                self.planned_demands, self.demand_noise_std, self.demand_noise\n",
    "            )\n",
    "\n",
    "        # Collect initial inventories from the graph\n",
    "        initial_inventories = []\n",
    "        for node in self.graph.nodes:\n",
    "            if node not in [\"S\", \"D\"]:\n",
    "                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n",
    "\n",
    "        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n",
    "\n",
    "        self.state = {\n",
    "            \"inventory_levels\": initial_inventories.astype(np.float32),\n",
    "            \"planned_demand\": self.planned_demands,\n",
    "            \"actual_demand\": self.actual_demands,\n",
    "            \"current_demand\": self.actual_demands[0],\n",
    "            \"backlog_levels\": np.zeros(num_nodes),\n",
    "            \"order_queue_status\": np.zeros(num_nodes),\n",
    "        }\n",
    "\n",
    "        # Prep to save the data\n",
    "        self.inventory = initial_inventories\n",
    "        self.stock_history = [self.inventory.tolist()]\n",
    "        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n",
    "\n",
    "        # Kaggle mode\n",
    "        self.kaggle = kaggle\n",
    "\n",
    "        # Render mode\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_initialized = False\n",
    "\n",
    "    # Defining the step function\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one step in the environment.\n",
    "        Starts by processing the orders and updating the inventory levels for each node.\n",
    "        Then, it computes the reward based on the order costs and stock level.\n",
    "        Finally, it checks if the episode is done and returns the next state, reward, and whether the episode is done.\n",
    "        \"\"\"\n",
    "\n",
    "        # Returns the next state, reward and whether the episode is done\n",
    "        timestep = self.EP_LENGTH - self.episode_length\n",
    "\n",
    "        # num_nodes = len(self.graph.nodes) - 2\n",
    "\n",
    "        # Retrieve the current inventory levels\n",
    "        self.inventory = self.state[\"inventory_levels\"]\n",
    "        inventory_levels = np.copy(self.inventory)\n",
    "        reward = 0\n",
    "\n",
    "        # Retrieve the actual demand for the current timestep\n",
    "        self.current_demand = self.actual_demands[timestep].astype(np.float32)\n",
    "\n",
    "        # Add every first element of the order queues to the history\n",
    "        self.new_order = [self.order_quantities[i] for i in action]\n",
    "\n",
    "        # For visualization and history data\n",
    "        self.orders = np.array(\n",
    "            [\n",
    "                self.order_queues[node][0]\n",
    "                for node in self.graph.nodes\n",
    "                if node not in [\"S\", \"D\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Process the orders and update the inventory levels for each node\n",
    "        for node in self.graph.nodes:\n",
    "            if node not in [\"S\", \"D\"]:\n",
    "                node_index = self.node_to_index(node)\n",
    "\n",
    "                # Deduct costs for placing new orders\n",
    "                if self.new_order[node_index] > 0:\n",
    "                    reward -= self.order_cost + (\n",
    "                        self.new_order[node_index] * self.item_cost\n",
    "                    )\n",
    "\n",
    "                # Fulfill orders from the queue\n",
    "                order = self.order_queues[node].popleft()\n",
    "                inventory_levels[node_index] += order\n",
    "\n",
    "                # Attempt to meet current demand\n",
    "                node_demand = self.current_demand[node_index]\n",
    "                if inventory_levels[node_index] >= node_demand:\n",
    "                    # Enough stock to meet demand\n",
    "                    inventory_levels[node_index] -= node_demand\n",
    "                    reward += node_demand * self.item_prize\n",
    "                else:\n",
    "                    # Insufficient stock - add unmet demand to backlog and apply penalty\n",
    "                    unmet_demand = node_demand - inventory_levels[node_index]\n",
    "                    inventory_levels[node_index] -= node_demand - unmet_demand\n",
    "                    reward += (node_demand - unmet_demand) * self.item_prize\n",
    "                    reward -= self.stockout_cost * unmet_demand  # Apply stockout cost\n",
    "                    self.backlog_queues[node].append(unmet_demand)\n",
    "\n",
    "                    # Increment the stockout counter\n",
    "                    self.stock_out_counter += 1\n",
    "\n",
    "                # Process backlog with any remaining stock\n",
    "                while self.backlog_queues[node] and inventory_levels[node_index] > 0:\n",
    "                    backlog_demand = self.backlog_queues[node][0]\n",
    "                    if inventory_levels[node_index] >= backlog_demand:\n",
    "                        inventory_levels[node_index] -= backlog_demand\n",
    "                        reward += backlog_demand * self.item_prize\n",
    "                        self.backlog_queues[node].popleft()\n",
    "                    else:\n",
    "                        break  # Not enough stock to clear the backlog completely\n",
    "\n",
    "                # backlog penalty\n",
    "                reward -= self.stockout_cost * len(self.backlog_queues[node])\n",
    "\n",
    "                # Replenish order queue\n",
    "                self.order_queues[node].append(self.new_order[node_index])\n",
    "\n",
    "        if self.progressive_stock_cost == False:\n",
    "            # Compute the reward based on the order costs and stock level\n",
    "            reward -= np.sum(inventory_levels) * self.stock_cost\n",
    "        elif self.progressive_stock_cost == True:\n",
    "            reward -= np.sum(\n",
    "                [\n",
    "                    self.quadratic_stock_cost(self.stock_cost, inv)\n",
    "                    for inv in inventory_levels\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Penalty if the episode cannot be completed\n",
    "        if self.stock_out_counter >= self.stock_out_max:\n",
    "            reward -= (\n",
    "                self.episode_length * self.stockout_cost * (len(self.graph.nodes) - 2)\n",
    "            )\n",
    "\n",
    "        # Intermediate reward\n",
    "        if timestep % 2 == 0:\n",
    "            reward += self.intermediate_reward\n",
    "\n",
    "        # Update the reward\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Decrease the episode length\n",
    "        self.episode_length -= 1\n",
    "\n",
    "        inventory_levels = inventory_levels.flatten()\n",
    "        self.inventory = inventory_levels\n",
    "\n",
    "        self.state = {\n",
    "            \"inventory_levels\": inventory_levels.astype(np.float32),\n",
    "            \"planned_demand\": self.planned_demands,\n",
    "            \"actual_demand\": self.actual_demands,\n",
    "            \"current_demand\": self.actual_demands[timestep],\n",
    "            \"backlog_levels\": self.backlog_queues,\n",
    "            \"order_queue_status\": self.order_queues,\n",
    "        }\n",
    "\n",
    "        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n",
    "\n",
    "        obs = {\n",
    "            \"inventory_levels\": self.inventory.astype(np.float32),\n",
    "            \"current_demand\": self.actual_demands[timestep].astype(np.float32),\n",
    "            \"backlog_levels\": np.array(\n",
    "                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n",
    "            ),\n",
    "            \"order_queues\": np.array(\n",
    "                [\n",
    "                    list(self.order_queues[node])\n",
    "                    + [0] * (max_lead_time - len(self.order_queues[node]))\n",
    "                    for node in self.graph.nodes\n",
    "                    if node not in [\"S\", \"D\"]\n",
    "                ],\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "            \"lead_times\": np.array(\n",
    "                [\n",
    "                    len(self.order_queues[node])\n",
    "                    for node in self.graph.nodes\n",
    "                    if node not in [\"S\", \"D\"]\n",
    "                ],\n",
    "                dtype=np.int32,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Update the history data\n",
    "        self.reward_history.append(reward)\n",
    "        self.stock_history.append(list(self.inventory))\n",
    "\n",
    "        self.log_step_data(timestep, action, reward)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.episode_length == 0\n",
    "\n",
    "        # Check if episode is done\n",
    "        if self.stock_out_counter >= self.stock_out_max:\n",
    "\n",
    "            done = True\n",
    "\n",
    "        elif self.episode_length <= 0:\n",
    "\n",
    "            done = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            done = False\n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Check if the episode is truncated\n",
    "        truncated = False\n",
    "\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def quadratic_stock_cost(self, stock_cost, inventory_level):\n",
    "        \"\"\"\n",
    "        Quadratic stock cost function.\n",
    "        \"\"\"\n",
    "        return stock_cost * (inventory_level**2)  # Quadratic cost\n",
    "\n",
    "    def log_step_data(self, timestep, action, reward):\n",
    "\n",
    "        if not self.file_initialized:\n",
    "            with open(self.data_path, \"w\", newline=\"\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
    "                writer.writeheader()\n",
    "            self.file_initialized = True  # Mark as initialized\n",
    "\n",
    "        for n in range(len(self.inventory)):\n",
    "            node_name = self.get_node_name(n)\n",
    "            row = {\n",
    "                \"Time\": timestep + 1,\n",
    "                \"Node\": node_name,\n",
    "                \"Stock\": self.inventory[n],\n",
    "                \"Action\": self.new_order[n],\n",
    "                \"Demand\": self.current_demand[n],\n",
    "                \"Delivery\": self.orders[n],\n",
    "                \"Reward\": reward,\n",
    "                \"Total Reward\": self.total_reward,\n",
    "                \"Backlog\": len(self.backlog_queues[node_name]) > 0,\n",
    "            }\n",
    "\n",
    "            # Append the row to the CSV file\n",
    "            with open(self.data_path, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    def reward_function(self):\n",
    "        # TODO - Implement a custom reward function\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def render(self):\n",
    "        # Just check episode lenghth and only plot the last one when using matplotlib\n",
    "        if self.render_mode is not None:\n",
    "            if self.render_mode == \"human\":\n",
    "                self.render_human()\n",
    "\n",
    "    def render_human(self):\n",
    "        \"\"\"\n",
    "        Renders the environment in human mode.\n",
    "        Useful for debugging and visualization.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"\\nEpisode Information\")\n",
    "        print(f\"Episode Length: {self.EP_LENGTH - self.episode_length}\")\n",
    "        if len(self.stock_history) > 1:\n",
    "            print(f\"Stock Level (Previous Timestep): {self.stock_history[-2]}\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Stock Level (Previous Timestep): No previous timestep data available\"\n",
    "            )\n",
    "        print(f\"Stock Level: {self.inventory}\")\n",
    "        print(\n",
    "            f\"Planned Demand: {self.planned_demands[self.EP_LENGTH - self.episode_length - 1]}\"\n",
    "        )\n",
    "        print(f\"Actual Demand: {self.current_demand}\")\n",
    "        print(f\"Action: {self.new_order}\")\n",
    "        print(f\"Deliveries: {self.orders}\")\n",
    "        # print(\n",
    "        #     f\"Previous Reward: {self.reward_history[self.EP_LENGTH - self.episode_length - 1]}\"\n",
    "        # )\n",
    "        print(f\"Step Reward: {self.reward_history[-1]}\")\n",
    "        print(f\"Total Reward: {self.total_reward}\")\n",
    "\n",
    "        print(\"\\nBacklog:\")\n",
    "        print([len(queue) > 0 for queue in self.backlog_queues.values()])\n",
    "        # pprint(self.backlog_queues, indent=4)\n",
    "\n",
    "        print(\"\\nOrder Queue:\")\n",
    "        pprint(self.order_queues, indent=4)\n",
    "        print()\n",
    "\n",
    "        # print(\"Stockout Cost: \", self.stockout_cost)\n",
    "        print(\"\\nStockout Counter: \", self.stock_out_counter)\n",
    "\n",
    "        return\n",
    "\n",
    "    def setup_network(self, network_config=None):\n",
    "        \"\"\"\n",
    "        Sets up the network graph based on the configuration provided.\n",
    "        \"\"\"\n",
    "        config = json.loads(network_config)\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        for node, attributes in config[\"nodes\"].items():\n",
    "            self.graph.add_node(node, **attributes)\n",
    "\n",
    "        # Add edges to the graph with lead times\n",
    "        for edge in config[\"edges\"]:\n",
    "            self.graph.add_edge(edge[\"source\"], edge[\"target\"], L=edge[\"L\"])\n",
    "\n",
    "    def render_network(self):\n",
    "        \"\"\"\n",
    "        Renders the network graph using NetworkX and Matplotlib.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Node Attributes:\")\n",
    "        for node, attributes in self.graph.nodes(data=True):\n",
    "            print(f\"Node {node}: {attributes}\")\n",
    "\n",
    "        pos = graphviz_layout(self.graph, prog=\"dot\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        nx.draw_networkx_nodes(self.graph, pos, node_size=700, node_color=\"lightblue\")\n",
    "        nx.draw_networkx_edges(\n",
    "            self.graph, pos, edgelist=self.graph.edges(), arrowstyle=\"->\", arrowsize=20\n",
    "        )\n",
    "        nx.draw_networkx_labels(self.graph, pos, font_size=12, font_family=\"sans-serif\")\n",
    "\n",
    "        edge_labels = nx.get_edge_attributes(self.graph, \"L\")\n",
    "        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n",
    "\n",
    "        plt.title(\"Supply Chain Network Graph\", fontsize=15)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def node_to_index(self, node):\n",
    "        \"\"\"\n",
    "        Returns the index of the node given its name.\n",
    "        \"\"\"\n",
    "        return list(self.graph.nodes).index(node)\n",
    "\n",
    "    def get_node_name(self, index):\n",
    "        \"\"\"\n",
    "        Returns the name of the node given its index.\n",
    "        \"\"\"\n",
    "        return list(self.graph.nodes)[index]\n",
    "\n",
    "    def planned_demand(self, demand_mean=10, demand_std=2, demand_prob=0.8):\n",
    "        \"\"\"\n",
    "        Generates planned demand for each edge in the network over the whole episode.\n",
    "        \"\"\"\n",
    "\n",
    "        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n",
    "\n",
    "        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n",
    "\n",
    "        for i, edge in enumerate(edges_leading_to_D):\n",
    "            for j in range(self.EP_LENGTH):\n",
    "                # Introduce a probability of having demand\n",
    "                if np.random.rand() < demand_prob:\n",
    "                    planned_demand[j, i] = int(\n",
    "                        np.random.normal(demand_mean, demand_std)\n",
    "                    )\n",
    "\n",
    "        return planned_demand\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def planned_demand_seasonality(\n",
    "        self,\n",
    "        demand_mean=10,\n",
    "        demand_std=2,\n",
    "        demand_prob=0.8,\n",
    "        season_length=50,\n",
    "        seasonality_strength=0.8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates planned demand for each edge in the network over the whole episode,\n",
    "        incorporating seasonality into the demand pattern.\n",
    "\n",
    "        Parameters:\n",
    "        - demand_mean: Mean of the demand.\n",
    "        - demand_std: Standard deviation of the demand.\n",
    "        - demand_prob: Probability of demand occurring.\n",
    "        - season_length: Length of the seasonal cycle.\n",
    "\n",
    "        Returns:\n",
    "        - planned_demand: 2D array of planned demand with seasonality.\n",
    "        \"\"\"\n",
    "\n",
    "        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n",
    "\n",
    "        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n",
    "\n",
    "        # Generate seasonality factors for the episode length\n",
    "        seasonality_factors = 1 + seasonality_strength * np.sin(\n",
    "            2 * np.pi * np.arange(self.EP_LENGTH) / season_length\n",
    "        )\n",
    "\n",
    "        for i, edge in enumerate(edges_leading_to_D):\n",
    "            for j in range(self.EP_LENGTH):\n",
    "                # Introduce a probability of having demand\n",
    "                if np.random.rand() < demand_prob:\n",
    "                    # Apply seasonality to the mean demand\n",
    "                    seasonal_mean = demand_mean * seasonality_factors[j]\n",
    "                    planned_demand[j, i] = int(\n",
    "                        np.random.normal(seasonal_mean, demand_std)\n",
    "                    )\n",
    "\n",
    "        return planned_demand\n",
    "\n",
    "    def planned_demand_even(self, demand_mean=10, demand_std=2):\n",
    "        \"\"\"\n",
    "        Generates planned demand for each edge in the network over the whole episode.\n",
    "        The demand is distributed evenly, occurring only at fixed intervals (e.g., every fifth timestep).\n",
    "        \"\"\"\n",
    "\n",
    "        # Get edges leading to \"D\"\n",
    "        edges_leading_to_D = [edge for edge in self.graph.edges if edge[1] == \"D\"]\n",
    "\n",
    "        # Initialize demand array with zeros\n",
    "        planned_demand = np.zeros((self.EP_LENGTH, len(edges_leading_to_D)))\n",
    "\n",
    "        for i, edge in enumerate(edges_leading_to_D):\n",
    "            # Determine timesteps where demand occurs (e.g., every fifth timestep)\n",
    "            timesteps_with_demand = np.arange(0, self.EP_LENGTH, 5)\n",
    "\n",
    "            for j in timesteps_with_demand:\n",
    "                # Generate demand from a normal distribution\n",
    "                demand = max(\n",
    "                    0, np.random.normal(demand_mean, demand_std)\n",
    "                )  # Ensure non-negative demand\n",
    "                planned_demand[j, i] = int(demand)\n",
    "\n",
    "        return planned_demand\n",
    "\n",
    "    def actual_demand(self, planned_demand, demand_noise_std=2, demand_noise=2):\n",
    "        \"\"\"\n",
    "        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        actual_demand = np.copy(planned_demand)\n",
    "\n",
    "        for i in range(actual_demand.shape[0]):\n",
    "            for j in range(actual_demand.shape[1]):\n",
    "                # Add a small random noise to the planned demand\n",
    "                if planned_demand[i, j] > 0:\n",
    "                    noise = np.random.normal(demand_noise, demand_noise_std)\n",
    "                    # Ensure actual demand is not less than 0\n",
    "                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n",
    "\n",
    "        return actual_demand\n",
    "\n",
    "    def actual_demand_extremes(\n",
    "        self,\n",
    "        planned_demand,\n",
    "        demand_noise_std=2,\n",
    "        demand_noise=2,\n",
    "        n_multiplications=[3, 6],\n",
    "        size_multiplier=[3, 4],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates a random actual demand for each edge in the network based on the planned demand from the current timestep.\n",
    "        To create a more interesting scenario, the demand is multiplied by a random factor 3-5 times over the whole demand.\n",
    "        \"\"\"\n",
    "        actual_demand = np.copy(planned_demand)\n",
    "\n",
    "        # Add random noise to planned demand\n",
    "        for i in range(actual_demand.shape[0]):\n",
    "            for j in range(actual_demand.shape[1]):\n",
    "                if planned_demand[i, j] > 0:\n",
    "                    noise = np.random.normal(demand_noise, demand_noise_std)\n",
    "                    actual_demand[i, j] = int(max(0, actual_demand[i, j] + noise))\n",
    "\n",
    "        # Multiply demand by a random factor 3-5 times\n",
    "        num_multiplications = np.random.randint(\n",
    "            n_multiplications[0], n_multiplications[1]\n",
    "        )  # Randomly select between 3 and 5\n",
    "        for _ in range(num_multiplications):\n",
    "            # Select random indices\n",
    "            i = np.random.randint(0, actual_demand.shape[0])\n",
    "            j = np.random.randint(0, actual_demand.shape[1])\n",
    "\n",
    "            # Apply a random multiplier\n",
    "            if planned_demand[i, j] > 0:\n",
    "                multiplier = np.random.uniform(size_multiplier[0], size_multiplier[1])\n",
    "                actual_demand[i, j] = int(actual_demand[i, j] * multiplier)\n",
    "\n",
    "        return actual_demand\n",
    "\n",
    "    def order_queue(self, initial_order=10):\n",
    "        \"\"\"\n",
    "        Creates a dictionary for the order queues for each node in the network.\n",
    "        \"\"\"\n",
    "\n",
    "        order_queues = {}\n",
    "\n",
    "        for node in self.graph.nodes:\n",
    "\n",
    "            if node not in [\"S\", \"D\"]:\n",
    "                in_edges = list(self.graph.in_edges(node, data=True))\n",
    "\n",
    "                if in_edges:\n",
    "                    lead_time = in_edges[0][2][\"L\"]\n",
    "                    order_queues[node] = deque(\n",
    "                        [initial_order] + [0] * (lead_time - 1), maxlen=lead_time\n",
    "                    )\n",
    "\n",
    "        return order_queues\n",
    "\n",
    "    def backlog_queue(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary for the backlog queues for each node in the network.\n",
    "        \"\"\"\n",
    "\n",
    "        backlog_queues = {}\n",
    "\n",
    "        for node in self.graph.nodes:\n",
    "            if node not in [\"S\", \"D\"]:\n",
    "\n",
    "                in_edges = list(self.graph.in_edges(node, data=True))\n",
    "                if in_edges:\n",
    "                    backlog_queues[node] = deque()\n",
    "\n",
    "        return backlog_queues\n",
    "\n",
    "    def save_state(self):\n",
    "        \"\"\"\n",
    "        Saves the current state of the environment.\n",
    "        Used for greedy algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"episode_length\": self.episode_length,\n",
    "            \"inventory\": np.copy(self.inventory),\n",
    "            \"total_reward\": self.total_reward,\n",
    "            \"state\": self.state,\n",
    "            \"order_queues\": {k: deque(v) for k, v in self.order_queues.items()},\n",
    "            \"backlog_queues\": {k: deque(v) for k, v in self.backlog_queues.items()},\n",
    "        }\n",
    "\n",
    "    def load_state(self, saved_state):\n",
    "        \"\"\"\n",
    "        Loads the state of the environment.\n",
    "        Used for greedy algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        self.episode_length = saved_state[\"episode_length\"]\n",
    "        self.inventory = saved_state[\"inventory\"]\n",
    "        self.total_reward = saved_state[\"total_reward\"]\n",
    "        self.state = saved_state[\"state\"]\n",
    "        self.order_queues = {\n",
    "            k: deque(v) for k, v in saved_state[\"order_queues\"].items()\n",
    "        }\n",
    "        self.backlog_queues = {\n",
    "            k: deque(v) for k, v in saved_state[\"backlog_queues\"].items()\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)  # Reset the seed\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "        # Reset the episode length\n",
    "        self.episode_length = self.EP_LENGTH\n",
    "\n",
    "        self.file_initialized = False\n",
    "\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # Reset the network\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.setup_network(self.network_config)\n",
    "\n",
    "        num_nodes = len(self.graph.nodes) - 2\n",
    "\n",
    "        # Order delay and backlog queue\n",
    "        self.order_queues = self.order_queue(initial_order=self.order_quantities[1])\n",
    "        self.backlog_queues = self.backlog_queue()\n",
    "\n",
    "        self.stock_out_counter = 0\n",
    "\n",
    "        # Define the initial state\n",
    "\n",
    "        if self.seasonality == True:\n",
    "\n",
    "            self.planned_demands = self.planned_demand_seasonality(\n",
    "                self.demand_mean, self.demand_std, self.demand_prob\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.planned_demands = self.planned_demand(\n",
    "                self.demand_mean, self.demand_std, self.demand_prob\n",
    "            )\n",
    "\n",
    "        if self.extreme == True:\n",
    "\n",
    "            self.actual_demands = self.actual_demand_extremes(\n",
    "                self.planned_demands, self.demand_noise_std, self.demand_noise\n",
    "            ).astype(np.float32)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.actual_demands = self.actual_demand(\n",
    "                self.planned_demands, self.demand_noise_std, self.demand_noise\n",
    "            ).astype(np.float32)\n",
    "\n",
    "        self.current_demand = self.actual_demands[0].astype(np.float32)\n",
    "\n",
    "        # Collect initial inventories from the graph\n",
    "        initial_inventories = []\n",
    "        for node in self.graph.nodes:\n",
    "            if node not in [\"S\", \"D\"]:\n",
    "                initial_inventories.append(self.graph.nodes[node].get(\"I\", 0))\n",
    "\n",
    "        # Convert to numpy array\n",
    "        initial_inventories = np.array(initial_inventories, dtype=np.float32).flatten()\n",
    "\n",
    "        self.state = {\n",
    "            \"inventory_levels\": initial_inventories,\n",
    "            \"planned_demand\": self.planned_demands,\n",
    "            \"actual_demand\": self.current_demand,\n",
    "        }\n",
    "\n",
    "        max_lead_time = max([data[\"L\"] for _, _, data in self.graph.edges(data=True)])\n",
    "        obs = {\n",
    "            \"inventory_levels\": self.inventory.astype(np.float32),\n",
    "            \"current_demand\": self.actual_demands[0].astype(np.float32),\n",
    "            \"backlog_levels\": np.array(\n",
    "                [len(queue) for queue in self.backlog_queues.values()], dtype=np.float32\n",
    "            ),\n",
    "            \"order_queues\": np.array(\n",
    "                [\n",
    "                    list(self.order_queues[node])\n",
    "                    + [0] * (max_lead_time - len(self.order_queues[node]))\n",
    "                    for node in self.graph.nodes\n",
    "                    if node not in [\"S\", \"D\"]\n",
    "                ],\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "            \"lead_times\": np.array(\n",
    "                [\n",
    "                    len(self.order_queues[node])\n",
    "                    for node in self.graph.nodes\n",
    "                    if node not in [\"S\", \"D\"]\n",
    "                ],\n",
    "                dtype=np.int32,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Resetting history data\n",
    "        self.inventory = initial_inventories\n",
    "        self.stock_history = [self.inventory.tolist()]\n",
    "        self.reward_history = [np.sum(initial_inventories * self.stock_cost * -1)]\n",
    "\n",
    "        # Placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "class MultiDiscreteToBoxWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Converts a MultiDiscrete action space to a Box action space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(\n",
    "            env.action_space, MultiDiscrete\n",
    "        ), \"Environment must have a MultiDiscrete action space.\"\n",
    "        self.original_action_space = env.action_space\n",
    "        self.action_space = Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=self.original_action_space.nvec.shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def action(self, action):\n",
    "        # Convert continuous action (Box) back to discrete (MultiDiscrete)\n",
    "        discrete_action = np.round(\n",
    "            action * (self.original_action_space.nvec - 1)\n",
    "        ).astype(int)\n",
    "        return discrete_action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        # Optionally: Map discrete actions back to normalized (Box)\n",
    "        normalized_action = action / (self.original_action_space.nvec - 1)\n",
    "        return normalized_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a05f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:34:53.641247Z",
     "iopub.status.busy": "2024-12-04T19:34:53.640378Z",
     "iopub.status.idle": "2024-12-04T19:34:53.655261Z",
     "shell.execute_reply": "2024-12-04T19:34:53.653976Z"
    },
    "papermill": {
     "duration": 0.020883,
     "end_time": "2024-12-04T19:34:53.657514",
     "exception": false,
     "start_time": "2024-12-04T19:34:53.636631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration of the network\n",
    "\n",
    "with open(\"/kaggle/input/config-v0/network_config_v1.json\") as file:\n",
    "\n",
    "    network_config = file.read()\n",
    "\n",
    "\n",
    "\n",
    "EP_LENGTH = 100  # Length of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792683f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:34:53.665665Z",
     "iopub.status.busy": "2024-12-04T19:34:53.665353Z",
     "iopub.status.idle": "2024-12-04T19:34:53.697937Z",
     "shell.execute_reply": "2024-12-04T19:34:53.696953Z"
    },
    "papermill": {
     "duration": 0.039671,
     "end_time": "2024-12-04T19:34:53.700584",
     "exception": false,
     "start_time": "2024-12-04T19:34:53.660913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation order_queues has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_config(config_file):\n",
    "\n",
    "\n",
    "\n",
    "    with open(config_file, \"r\") as f:\n",
    "\n",
    "        config = json.load(f)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "def make_env(config_file=\"config.json\"):\n",
    "\n",
    "\n",
    "\n",
    "    config = load_config(config_file)\n",
    "\n",
    "\n",
    "\n",
    "    env = SS_Mngmt_Env(\n",
    "\n",
    "        network_config=network_config,\n",
    "\n",
    "        EP_LENGTH=EP_LENGTH,\n",
    "\n",
    "        render_mode=\"human\",\n",
    "\n",
    "        model_type=\"PPO\",\n",
    "\n",
    "        stockout_cost=config[\"stockout_cost\"],\n",
    "\n",
    "        order_cost=config[\"order_cost\"],\n",
    "\n",
    "        item_cost=config[\"item_cost\"],\n",
    "\n",
    "        stock_cost=config[\"stock_cost\"],\n",
    "\n",
    "        item_prize=config[\"item_prize\"],\n",
    "\n",
    "        progressive_stock_cost=config[\"progressive_stock_cost\"],\n",
    "\n",
    "        stock_out_max=config[\"stock_out_max\"],\n",
    "\n",
    "        order_quantities=config[\"order_quantities\"],\n",
    "\n",
    "        demand_mean=config[\"demand_mean\"],\n",
    "\n",
    "        demand_std=config[\"demand_std\"],\n",
    "\n",
    "        demand_noise=config[\"demand_noise\"],\n",
    "\n",
    "        demand_noise_std=config[\"demand_noise_std\"],\n",
    "\n",
    "        demand_prob=config[\"demand_prob\"],\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return Monitor(env)\n",
    "\n",
    "\n",
    "\n",
    "env = make_env(\"/kaggle/input/config-v0/env_config_v0.json\")\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027da47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:34:53.707941Z",
     "iopub.status.busy": "2024-12-04T19:34:53.707639Z",
     "iopub.status.idle": "2024-12-04T19:35:06.122938Z",
     "shell.execute_reply": "2024-12-04T19:35:06.122026Z"
    },
    "papermill": {
     "duration": 12.421539,
     "end_time": "2024-12-04T19:35:06.125090",
     "exception": false,
     "start_time": "2024-12-04T19:34:53.703551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_envs = 4\n",
    "\n",
    "env_config_path = \"/kaggle/input/config-v0/env_config_v0.json\"\n",
    "\n",
    "\n",
    "\n",
    "vec_env = SubprocVecEnv([lambda: make_env(env_config_path) for _ in range(num_envs)])\n",
    "\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371d9171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:35:06.133308Z",
     "iopub.status.busy": "2024-12-04T19:35:06.132368Z",
     "iopub.status.idle": "2024-12-05T03:02:31.130060Z",
     "shell.execute_reply": "2024-12-05T03:02:31.129079Z"
    },
    "papermill": {
     "duration": 26845.009248,
     "end_time": "2024-12-05T03:02:31.137457",
     "exception": false,
     "start_time": "2024-12-04T19:35:06.128209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-04 19:35:06,137] A new study created in memory with name: no-name-2f1e4b39-92df-4ecf-be0c-0d664cca97a5\n",
      "[I 2024-12-04 19:49:56,956] Trial 0 finished with value: -7009019.209999999 and parameters: {'learning_rate': 0.0001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 16, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 0 with value: -7009019.209999999.\n",
      "[I 2024-12-04 20:04:09,176] Trial 1 finished with value: -73885.11600000001 and parameters: {'learning_rate': 0.0001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 32, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 1 with value: -73885.11600000001.\n",
      "[I 2024-12-04 20:18:22,777] Trial 2 finished with value: -19581146.369999997 and parameters: {'learning_rate': 0.0001, 'gamma': 0.99, 'ent_coef': 0.01, 'vf_coef': 0.5, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.99}. Best is trial 1 with value: -73885.11600000001.\n",
      "[I 2024-12-04 20:32:29,273] Trial 3 finished with value: -57703.464 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 3 with value: -57703.464.\n",
      "[I 2024-12-04 20:48:26,763] Trial 4 finished with value: -5286708.364 and parameters: {'learning_rate': 0.0005, 'gamma': 0.99, 'ent_coef': 0.05, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 1.0, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 3 with value: -57703.464.\n",
      "[I 2024-12-04 21:04:22,634] Trial 5 finished with value: -63611.795999999995 and parameters: {'learning_rate': 0.0005, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 1.0, 'use_rms_prop': False, 'gae_lambda': 0.99}. Best is trial 3 with value: -57703.464.\n",
      "[I 2024-12-04 21:18:51,553] Trial 6 finished with value: -1224972.874 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 32, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 3 with value: -57703.464.\n",
      "[I 2024-12-04 21:34:57,482] Trial 7 finished with value: -56559.52 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 21:49:25,318] Trial 8 finished with value: -1133290.632 and parameters: {'learning_rate': 0.001, 'gamma': 0.99, 'ent_coef': 0.1, 'vf_coef': 0.8, 'n_steps': 32, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 22:03:34,287] Trial 9 finished with value: -309929.53 and parameters: {'learning_rate': 0.0005, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 64, 'max_grad_norm': 1.0, 'use_rms_prop': False, 'gae_lambda': 0.99}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 22:19:22,724] Trial 10 finished with value: -14988546.180000002 and parameters: {'learning_rate': 0.001, 'gamma': 0.99, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 1.0, 'use_rms_prop': True, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 22:33:47,328] Trial 11 finished with value: -2530866.48 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 0.8, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': True, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 22:49:02,364] Trial 12 finished with value: -19454812.476 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.01, 'vf_coef': 1.0, 'n_steps': 16, 'max_grad_norm': 0.5, 'use_rms_prop': True, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 23:03:29,393] Trial 13 finished with value: -57976.653999999995 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 23:19:36,968] Trial 14 finished with value: -19573932.564 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 0.8, 'n_steps': 8, 'max_grad_norm': 0.5, 'use_rms_prop': True, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 23:34:03,876] Trial 15 finished with value: -19449307.28 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.5, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-04 23:50:20,815] Trial 16 finished with value: -409577.56399999995 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 00:05:35,580] Trial 17 finished with value: -74503.93000000001 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 1.0, 'n_steps': 16, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 00:21:40,601] Trial 18 finished with value: -19855283.810000002 and parameters: {'learning_rate': 0.0001, 'gamma': 0.99, 'ent_coef': 0.1, 'vf_coef': 0.5, 'n_steps': 8, 'max_grad_norm': 1.0, 'use_rms_prop': True, 'gae_lambda': 0.95}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 00:35:54,911] Trial 19 finished with value: -19031163.342 and parameters: {'learning_rate': 0.0005, 'gamma': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.8, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 00:50:03,923] Trial 20 finished with value: -422068.09799999994 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.95}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 01:04:33,876] Trial 21 finished with value: -242614.79000000004 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 01:18:53,475] Trial 22 finished with value: -198708.21399999998 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 01:33:13,757] Trial 23 finished with value: -83777.03 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 01:47:34,326] Trial 24 finished with value: -1063210.264 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 02:03:51,804] Trial 25 finished with value: -75115.054 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.05, 'vf_coef': 0.5, 'n_steps': 8, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.8}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 02:19:05,545] Trial 26 finished with value: -10651668.378 and parameters: {'learning_rate': 0.001, 'gamma': 0.99, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 16, 'max_grad_norm': 1.0, 'use_rms_prop': True, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 02:33:36,222] Trial 27 finished with value: -60405.846 and parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 32, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.99}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 02:47:36,621] Trial 28 finished with value: -56989.466 and parameters: {'learning_rate': 0.0001, 'gamma': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.8, 'n_steps': 64, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n",
      "[I 2024-12-05 03:02:31,125] Trial 29 finished with value: -19504641.61 and parameters: {'learning_rate': 0.0001, 'gamma': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.8, 'n_steps': 16, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}. Best is trial 7 with value: -56559.52.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.001, 'gamma': 0.95, 'ent_coef': 0.1, 'vf_coef': 1.0, 'n_steps': 8, 'max_grad_norm': 0.5, 'use_rms_prop': False, 'gae_lambda': 0.9}\n"
     ]
    }
   ],
   "source": [
    "def optimize_hyperparams(trial):\n",
    "\n",
    "    # Tune hyperparameters\n",
    "\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-4, 5e-4, 1e-3])\n",
    "\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.95, 0.99])\n",
    "\n",
    "    ent_coef = trial.suggest_categorical(\"ent_coef\", [0.01, 0.05, 0.1])\n",
    "\n",
    "    vf_coef = trial.suggest_categorical(\"vf_coef\", [0.5, 0.8, 1.0])\n",
    "\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64])\n",
    "\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.5, 1.0])\n",
    "\n",
    "    use_rms_prop = trial.suggest_categorical(\"use_rms_prop\", [True, False])\n",
    "\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.95, 0.99])\n",
    "\n",
    "    \n",
    "\n",
    "    # Create and train the model\n",
    "\n",
    "    model = A2C(\n",
    "\n",
    "        \"MultiInputPolicy\",\n",
    "\n",
    "        vec_env,\n",
    "\n",
    "        learning_rate=lr,\n",
    "\n",
    "        gamma=gamma,\n",
    "\n",
    "        ent_coef=ent_coef,\n",
    "\n",
    "        vf_coef=vf_coef,\n",
    "\n",
    "        n_steps=n_steps,\n",
    "\n",
    "        max_grad_norm=max_grad_norm,\n",
    "\n",
    "        gae_lambda=gae_lambda,\n",
    "\n",
    "        use_rms_prop=use_rms_prop,\n",
    "\n",
    "        verbose=0,\n",
    "\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=750_000)\n",
    "\n",
    "    \n",
    "\n",
    "    # Evaluate the model\n",
    "\n",
    "    rewards, _ = evaluate_policy(model, env, n_eval_episodes=5, return_episode_rewards=True)\n",
    "\n",
    "    return sum(rewards) / len(rewards)\n",
    "\n",
    "\n",
    "\n",
    "# Perform the optimization\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study.optimize(optimize_hyperparams, n_trials=30)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b631410c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T03:02:31.152024Z",
     "iopub.status.busy": "2024-12-05T03:02:31.148307Z",
     "iopub.status.idle": "2024-12-05T03:02:31.158493Z",
     "shell.execute_reply": "2024-12-05T03:02:31.157405Z"
    },
    "papermill": {
     "duration": 0.018779,
     "end_time": "2024-12-05T03:02:31.160973",
     "exception": false,
     "start_time": "2024-12-05T03:02:31.142194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "# Save the study to a file\n",
    "\n",
    "with open(f\"/kaggle/working/A2C_optuna_study_{now.strftime('%Y-%m-%d_%H_%M')}.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(study, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6207537,
     "sourceId": 10071183,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26882.355413,
   "end_time": "2024-12-05T03:02:33.787612",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T19:34:31.432199",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
