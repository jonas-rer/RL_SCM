{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "Install required dependencies.\n",
    "\n",
    "- 'stable baselines' --> Reinforcement Learning library used for training the agent. Implementation of RL algorithms. Make sure to install the package 'stable-baselines3' for the latest version. Other versions are not installable.\n",
    "- 'gym' --> OpenAI library for creating and training environments.\n",
    "\n",
    "Link to stable baselines doucmentation: https://stable-baselines.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable baselines install\n",
    "# %pip install stable-baselines3[extra]\n",
    "\n",
    "# Alternatively:\n",
    "# %pip install stable-baselines3\n",
    "\n",
    "# %pip install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the environment\n",
    "\n",
    "There is the key difference between simulated and real environments. As we will be working with a simulated environment, the OpenAI 'gym' environment will be used.\n",
    "\n",
    "In the case of the thesis this will be a custom environment that is coded separately.\n",
    "\n",
    "## OpenAI Gym Spaces\n",
    "\n",
    "These are the different types of spaces that can be used in the gym environment.\n",
    "\n",
    "- Box: A n-dimensional box that contains every point in the space.\n",
    "- Discrete: set of items\n",
    "- Tuple: A tuple of elements from different spaces.\n",
    "- Dict: A dictionary that maps keys to spaces.\n",
    "- MultiBinary: A binary space of n bits.\n",
    "- MultiDiscrete: A set of integers, with each integer in a different range.\n",
    "\n",
    "## Custom Environment\n",
    "\n",
    "#TODO\n",
    "\n",
    "## Load the environment\n",
    "\n",
    "When using the pre coded environment from the package this can be done by defining the name of the environment and then passing is through the gym.make() function.\n",
    "\n",
    "The envorinment can be visualized by using the render() function.\n",
    "\n",
    "To teste the environment a simple loop could be created.\n",
    "\n",
    "## Understanding the environment\n",
    "\n",
    "The environment has the following attributes:\n",
    "\n",
    "- action_space: The action space of the environment. Understand the range of actions the agent can take.\n",
    "- observation_space: The observation space of the environment. Get an understanding of the possbile values for the observation state.\n",
    "- reward_range: The reward range of the environment. How is the reward defined for the agent.\n",
    "\n",
    "It is important to document the environment somewhere. To make it as clear as possible to later users what they are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "\n",
    "There are many different types of learning algorithms.\n",
    "\n",
    "Mosed algorithms in practise and where most research happens is in model free learning.\n",
    "\n",
    "This website: https://spinningup.openai.com/en/latest/ --> provides a good overview of the different algorithms. As well as very comprehensive explanations of key concepts of reinforcement learning.\n",
    "\n",
    "The selected algorithm should fit to the observation state as well as the action space of the environment. Further information about this can be found in the stable-baselines documentation.\n",
    "\n",
    "- Discrete single process: DQN\n",
    "- Discrete multi process: A2C, PPO\n",
    "- Continuous single process: SAC or TD3\n",
    "- Continuous multi process: A2C, PPO\n",
    "\n",
    "For training with GPU support, the respective pytorch or tensorflow version should be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "Core metrics to look at:\n",
    "\n",
    "- Average reward\n",
    "- Avg. episode length\n",
    "\n",
    "\n",
    "For analysis of the training process, the tensorboard can be used. This can be done by using the tensorboard callback from stable baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Callbacks, Alternative Algorithms and Architecture\n",
    "\n",
    "For more complicated taks and larger models it makes sense to define a threshold for the reward.\n",
    "\n",
    "This can be done by using the callback function from stable baselines.\n",
    "\n",
    "By limiting the reward, the training can be stopped when the agent has reached a certain level of performance.\n",
    "\n",
    "This guarantees that the agent does not overfit the training data and the model does not become too complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-scm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
